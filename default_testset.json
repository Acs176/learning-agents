{"user_input":"what is flight_assistant in langgraph?","reference_contexts":["search: boost: 2 tags: - agent hide: - tags Multi-agent A single agent might struggle if it needs to specialize in multiple domains or manage many tools. To tackle this, you can break your agent into smaller, independent agents and composing them into a multi-agent system. In multi-agent systems, agents need to communicate between each other. They do so via handoffs \u2014 a primitive that describes which agent to hand control to and the payload to send to that agent. Two of the most popular multi-agent architectures are: supervisor \u2014 individual agents are coordinated by a central supervisor agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements. swarm \u2014 agents dynamically hand off control to one another based on their specializations. The system remembers which agent was last active, ensuring that on subsequent interactions, the conversation resumes with that agent. Supervisor Supervisor Use langgraph-supervisor library to create a supervisor multi-agent system: bash pip install langgraph-supervisor ```python from langchain_openai import ChatOpenAI from langgraph.prebuilt import create_react_agent highlight-next-line from langgraph_supervisor import create_supervisor def book_hotel(hotel_name: str): \"\"\"Book a hotel\"\"\" return f\"Successfully booked a stay at {hotel_name}.\" def book_flight(from_airport: str, to_airport: str): \"\"\"Book a flight\"\"\" return f\"Successfully booked a flight from {from_airport} to {to_airport}.\" flight_assistant = create_react_agent( model=\"openai:gpt-4o\", tools=[book_flight], prompt=\"You are a flight booking assistant\", # highlight-next-line name=\"flight_assistant\" ) hotel_assistant = create_react_agent( model=\"openai:gpt-4o\", tools=[book_hotel], prompt=\"You are a hotel booking assistant\", # highlight-next-line name=\"hotel_assistant\" ) highlight-next-line supervisor = create_supervisor( agents=[flight_assistant, hotel_assistant], model=ChatOpenAI(model=\"gpt-4o\"), prompt=( \"You manage a hotel booking assistant and a\" \"flight booking assistant. Assign work to them.\" ) ).compile() for chunk in supervisor.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\" } ] } ): print(chunk) print(\"\\n\") ``` Swarm Swarm Use langgraph-swarm library to create a swarm multi-agent system: bash pip install langgraph-swarm ```python from langgraph.prebuilt import create_react_agent highlight-next-line from langgraph_swarm import create_swarm, create_handoff_tool transfer_to_hotel_assistant = create_handoff_tool( agent_name=\"hotel_assistant\", description=\"Transfer user to the hotel-booking assistant.\", ) transfer_to_flight_assistant = create_handoff_tool( agent_name=\"flight_assistant\", description=\"Transfer user to the flight-booking assistant.\", ) flight_assistant = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", # highlight-next-line tools=[book_flight, transfer_to_hotel_assistant], prompt=\"You are a flight booking assistant\", # highlight-next-line name=\"flight_assistant\" ) hotel_assistant = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", # highlight-next-line tools=[book_hotel, transfer_to_flight_assistant], prompt=\"You are a hotel booking assistant\", # highlight-next-line name=\"hotel_assistant\" ) highlight-next-line swarm = create_swarm( agents=[flight_assistant, hotel_assistant], default_active_agent=\"flight_assistant\" ).compile() for chunk in swarm.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\" } ] } ): print(chunk) print(\"\\n\") ``` Handoffs A common pattern in multi-agent interactions is handoffs, where one agent hands off control to another. Handoffs allow you to specify: destination: target agent to navigate to payload: information to pass to that agent This is used both by langgraph-supervisor (supervisor hands off to individual agents) and langgraph-swarm (an individual agent can hand off to other agents). To implement handoffs with create_react_agent, you need to: Create a special tool that can transfer control to a different agent python def transfer_to_bob(): \"\"\"Transfer to bob.\"\"\" return Command( # name of the agent (node) to go to # highlight-next-line goto=\"bob\", # data to send to the agent # highlight-next-line update={\"messages\": [...]}, # indicate to LangGraph that we need to navigate to # agent node in a parent graph # highlight-next-line graph=Command.PARENT, ) Create individual agents that have access to handoff tools: python flight_assistant = create_react_agent( ..., tools=[book_flight, transfer_to_hotel_assistant] ) hotel_assistant = create_react_agent( ..., tools=[book_hotel, transfer_to_flight_assistant] ) Define a parent graph that contains individual agents as nodes: python from langgraph.graph import StateGraph, MessagesState multi_agent_graph = ( StateGraph(MessagesState) .add_node(flight_assistant) .add_node(hotel_assistant) ... ) Putting this together, here is how you can implement a simple multi-agent system with two agents \u2014 a flight booking assistant and a hotel booking assistant: ```python from typing import Annotated from langchain_core.tools import tool, InjectedToolCallId from langgraph.prebuilt import create_react_agent, InjectedState from langgraph.graph import StateGraph, START, MessagesState from langgraph.types import Command def create_handoff_tool(*, agent_name: str, description: str | None = None): name = f\"transfer_to_{agent_name}\" description = description or f\"Transfer to {agent_name}\" @tool(name, description=description) def handoff_tool( # highlight-next-line state: Annotated[MessagesState, InjectedState], # (1)! # highlight-next-line tool_call_id: Annotated[str, InjectedToolCallId], ) -> Command: tool_message = { \"role\": \"tool\", \"content\": f\"Successfully transferred to {agent_name}\", \"name\": name, \"tool_call_id\": tool_call_id, } return Command( # (2)! # highlight-next-line goto=agent_name, # (3)! # highlight-next-line update={\"messages\": state[\"messages\"] + [tool_message]}, # (4)! # highlight-next-line graph=Command.PARENT, # (5)! ) return handoff_tool Handoffs transfer_to_hotel_assistant = create_handoff_tool( agent_name=\"hotel_assistant\", description=\"Transfer user to the hotel-booking assistant.\", ) transfer_to_flight_assistant = create_handoff_tool( agent_name=\"flight_assistant\", description=\"Transfer user to the flight-booking assistant.\", )"],"reference":"flight_assistant is a flight booking assistant created using the langgraph framework. It is designed to handle flight bookings and is defined with a specific model and tools, such as the ability to book flights.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"What is the significance of the McKittrick Hotel in the context of multi-agent systems?","reference_contexts":["search: boost: 2 tags: - agent hide: - tags Multi-agent A single agent might struggle if it needs to specialize in multiple domains or manage many tools. To tackle this, you can break your agent into smaller, independent agents and composing them into a multi-agent system. In multi-agent systems, agents need to communicate between each other. They do so via handoffs \u2014 a primitive that describes which agent to hand control to and the payload to send to that agent. Two of the most popular multi-agent architectures are: supervisor \u2014 individual agents are coordinated by a central supervisor agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements. swarm \u2014 agents dynamically hand off control to one another based on their specializations. The system remembers which agent was last active, ensuring that on subsequent interactions, the conversation resumes with that agent. Supervisor Supervisor Use langgraph-supervisor library to create a supervisor multi-agent system: bash pip install langgraph-supervisor ```python from langchain_openai import ChatOpenAI from langgraph.prebuilt import create_react_agent highlight-next-line from langgraph_supervisor import create_supervisor def book_hotel(hotel_name: str): \"\"\"Book a hotel\"\"\" return f\"Successfully booked a stay at {hotel_name}.\" def book_flight(from_airport: str, to_airport: str): \"\"\"Book a flight\"\"\" return f\"Successfully booked a flight from {from_airport} to {to_airport}.\" flight_assistant = create_react_agent( model=\"openai:gpt-4o\", tools=[book_flight], prompt=\"You are a flight booking assistant\", # highlight-next-line name=\"flight_assistant\" ) hotel_assistant = create_react_agent( model=\"openai:gpt-4o\", tools=[book_hotel], prompt=\"You are a hotel booking assistant\", # highlight-next-line name=\"hotel_assistant\" ) highlight-next-line supervisor = create_supervisor( agents=[flight_assistant, hotel_assistant], model=ChatOpenAI(model=\"gpt-4o\"), prompt=( \"You manage a hotel booking assistant and a\" \"flight booking assistant. Assign work to them.\" ) ).compile() for chunk in supervisor.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\" } ] } ): print(chunk) print(\"\\n\") ``` Swarm Swarm Use langgraph-swarm library to create a swarm multi-agent system: bash pip install langgraph-swarm ```python from langgraph.prebuilt import create_react_agent highlight-next-line from langgraph_swarm import create_swarm, create_handoff_tool transfer_to_hotel_assistant = create_handoff_tool( agent_name=\"hotel_assistant\", description=\"Transfer user to the hotel-booking assistant.\", ) transfer_to_flight_assistant = create_handoff_tool( agent_name=\"flight_assistant\", description=\"Transfer user to the flight-booking assistant.\", ) flight_assistant = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", # highlight-next-line tools=[book_flight, transfer_to_hotel_assistant], prompt=\"You are a flight booking assistant\", # highlight-next-line name=\"flight_assistant\" ) hotel_assistant = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", # highlight-next-line tools=[book_hotel, transfer_to_flight_assistant], prompt=\"You are a hotel booking assistant\", # highlight-next-line name=\"hotel_assistant\" ) highlight-next-line swarm = create_swarm( agents=[flight_assistant, hotel_assistant], default_active_agent=\"flight_assistant\" ).compile() for chunk in swarm.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\" } ] } ): print(chunk) print(\"\\n\") ``` Handoffs A common pattern in multi-agent interactions is handoffs, where one agent hands off control to another. Handoffs allow you to specify: destination: target agent to navigate to payload: information to pass to that agent This is used both by langgraph-supervisor (supervisor hands off to individual agents) and langgraph-swarm (an individual agent can hand off to other agents). To implement handoffs with create_react_agent, you need to: Create a special tool that can transfer control to a different agent python def transfer_to_bob(): \"\"\"Transfer to bob.\"\"\" return Command( # name of the agent (node) to go to # highlight-next-line goto=\"bob\", # data to send to the agent # highlight-next-line update={\"messages\": [...]}, # indicate to LangGraph that we need to navigate to # agent node in a parent graph # highlight-next-line graph=Command.PARENT, ) Create individual agents that have access to handoff tools: python flight_assistant = create_react_agent( ..., tools=[book_flight, transfer_to_hotel_assistant] ) hotel_assistant = create_react_agent( ..., tools=[book_hotel, transfer_to_flight_assistant] ) Define a parent graph that contains individual agents as nodes: python from langgraph.graph import StateGraph, MessagesState multi_agent_graph = ( StateGraph(MessagesState) .add_node(flight_assistant) .add_node(hotel_assistant) ... ) Putting this together, here is how you can implement a simple multi-agent system with two agents \u2014 a flight booking assistant and a hotel booking assistant: ```python from typing import Annotated from langchain_core.tools import tool, InjectedToolCallId from langgraph.prebuilt import create_react_agent, InjectedState from langgraph.graph import StateGraph, START, MessagesState from langgraph.types import Command def create_handoff_tool(*, agent_name: str, description: str | None = None): name = f\"transfer_to_{agent_name}\" description = description or f\"Transfer to {agent_name}\" @tool(name, description=description) def handoff_tool( # highlight-next-line state: Annotated[MessagesState, InjectedState], # (1)! # highlight-next-line tool_call_id: Annotated[str, InjectedToolCallId], ) -> Command: tool_message = { \"role\": \"tool\", \"content\": f\"Successfully transferred to {agent_name}\", \"name\": name, \"tool_call_id\": tool_call_id, } return Command( # (2)! # highlight-next-line goto=agent_name, # (3)! # highlight-next-line update={\"messages\": state[\"messages\"] + [tool_message]}, # (4)! # highlight-next-line graph=Command.PARENT, # (5)! ) return handoff_tool Handoffs transfer_to_hotel_assistant = create_handoff_tool( agent_name=\"hotel_assistant\", description=\"Transfer user to the hotel-booking assistant.\", ) transfer_to_flight_assistant = create_handoff_tool( agent_name=\"flight_assistant\", description=\"Transfer user to the flight-booking assistant.\", )"],"reference":"The McKittrick Hotel is mentioned as part of a user request in a multi-agent system example, where a user asks to book a flight and a stay at the hotel. It illustrates how agents can handle specific tasks, such as hotel bookings, within a multi-agent architecture.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"What is the significance of the McKittrick Hotel in the context of booking a stay?","reference_contexts":["Simple agent tools def book_hotel(hotel_name: str): \"\"\"Book a hotel\"\"\" return f\"Successfully booked a stay at {hotel_name}.\" def book_flight(from_airport: str, to_airport: str): \"\"\"Book a flight\"\"\" return f\"Successfully booked a flight from {from_airport} to {to_airport}.\" Define agents flight_assistant = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", # highlight-next-line tools=[book_flight, transfer_to_hotel_assistant], prompt=\"You are a flight booking assistant\", # highlight-next-line name=\"flight_assistant\" ) hotel_assistant = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", # highlight-next-line tools=[book_hotel, transfer_to_flight_assistant], prompt=\"You are a hotel booking assistant\", # highlight-next-line name=\"hotel_assistant\" ) Define multi-agent graph multi_agent_graph = ( StateGraph(MessagesState) .add_node(flight_assistant) .add_node(hotel_assistant) .add_edge(START, \"flight_assistant\") .compile() ) Run the multi-agent graph for chunk in multi_agent_graph.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\" } ] } ): print(chunk) print(\"\\n\") ``` Access agent's state The Command primitive allows specifying a state update and a node transition as a single operation, making it useful for implementing handoffs. Name of the agent or node to hand off to. Take the agent's messages and add them to the parent's state as part of the handoff. The next agent will see the parent state. Indicate to LangGraph that we need to navigate to agent node in a parent multi-agent graph. !!! Note This handoff implementation assumes that: each agent receives overall message history (across all agents) in the multi-agent system as its input - each agent outputs its internal messages history to the overall message history of the multi-agent system Check out LangGraph [supervisor](https:\/\/github.com\/langchain-ai\/langgraph-supervisor-py#customizing-handoff-tools) and [swarm](https:\/\/github.com\/langchain-ai\/langgraph-swarm-py#customizing-handoff-tools) documentation to learn how to customize handoffs."],"reference":"The McKittrick Hotel is mentioned as part of a user's request to book a flight and a stay at the hotel, indicating its relevance in the context of hotel booking within the multi-agent graph system.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"How do I book a stay at the McKittrick Hotel using LangGraph?","reference_contexts":["Simple agent tools def book_hotel(hotel_name: str): \"\"\"Book a hotel\"\"\" return f\"Successfully booked a stay at {hotel_name}.\" def book_flight(from_airport: str, to_airport: str): \"\"\"Book a flight\"\"\" return f\"Successfully booked a flight from {from_airport} to {to_airport}.\" Define agents flight_assistant = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", # highlight-next-line tools=[book_flight, transfer_to_hotel_assistant], prompt=\"You are a flight booking assistant\", # highlight-next-line name=\"flight_assistant\" ) hotel_assistant = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", # highlight-next-line tools=[book_hotel, transfer_to_flight_assistant], prompt=\"You are a hotel booking assistant\", # highlight-next-line name=\"hotel_assistant\" ) Define multi-agent graph multi_agent_graph = ( StateGraph(MessagesState) .add_node(flight_assistant) .add_node(hotel_assistant) .add_edge(START, \"flight_assistant\") .compile() ) Run the multi-agent graph for chunk in multi_agent_graph.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\" } ] } ): print(chunk) print(\"\\n\") ``` Access agent's state The Command primitive allows specifying a state update and a node transition as a single operation, making it useful for implementing handoffs. Name of the agent or node to hand off to. Take the agent's messages and add them to the parent's state as part of the handoff. The next agent will see the parent state. Indicate to LangGraph that we need to navigate to agent node in a parent multi-agent graph. !!! Note This handoff implementation assumes that: each agent receives overall message history (across all agents) in the multi-agent system as its input - each agent outputs its internal messages history to the overall message history of the multi-agent system Check out LangGraph [supervisor](https:\/\/github.com\/langchain-ai\/langgraph-supervisor-py#customizing-handoff-tools) and [swarm](https:\/\/github.com\/langchain-ai\/langgraph-swarm-py#customizing-handoff-tools) documentation to learn how to customize handoffs."],"reference":"To book a stay at the McKittrick Hotel using LangGraph, you can utilize the hotel booking assistant agent. You would need to run the multi-agent graph and provide a message that includes your request, such as 'book a flight from BOS to JFK and a stay at McKittrick Hotel'. The hotel assistant will then process this request and confirm the booking.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"What about new york?","reference_contexts":["Long-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. Manage message history Long conversations can exceed the LLM's context window. Common solutions are: Summarization: Maintain a running summary of the conversation Trimming: Remove first or last N messages in the history This allows the agent to keep track of the conversation without exceeding the LLM's context window. To manage message history, specify pre_model_hook \u2014 a function (node) that will always run before calling the language model. Summarize message history To summarize message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with a prebuilt SummarizationNode: ```python from langchain_anthropic import ChatAnthropic from langmem.short_term import SummarizationNode from langchain_core.messages.utils import count_tokens_approximately from langgraph.prebuilt import create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.checkpoint.memory import InMemorySaver from typing import Any model = ChatAnthropic(model=\"claude-3-7-sonnet-latest\") summarization_node = SummarizationNode( # (1)! token_counter=count_tokens_approximately, model=model, max_tokens=384, max_summary_tokens=128, output_messages_key=\"llm_input_messages\", ) class State(AgentState): # NOTE: we're adding this key to keep track of previous summary information # to make sure we're not summarizing on every LLM call # highlight-next-line context: dict[str, Any] # (2)! checkpointer = InMemorySaver() # (3)! agent = create_react_agent( model=model, tools=tools, # highlight-next-line pre_model_hook=summarization_node, # (4)! # highlight-next-line state_schema=State, # (5)! checkpointer=checkpointer, ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The context key is added to the agent's state. The key contains book-keeping information for the summarization node. It is used to keep track of the last summary information and ensure that the agent doesn't summarize on every LLM call, which can be inefficient. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. The pre_model_hook is set to the SummarizationNode. This node will summarize the message history before sending it to the LLM. The summarization node will automatically handle the summarization process and update the agent's state with the new summary. You can replace this with a custom implementation if you prefer. Please see the [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent] API reference for more details. The state_schema is set to the State class, which is the custom state that contains an extra context key. Trim message history To trim message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with trim_messages function: ```python highlight-next-line from langchain_core.messages.utils import ( # highlight-next-line trim_messages, # highlight-next-line count_tokens_approximately highlight-next-line ) from langgraph.prebuilt import create_react_agent This function will be called every time before the node that calls LLM def pre_model_hook(state): trimmed_messages = trim_messages( state[\"messages\"], strategy=\"last\", token_counter=count_tokens_approximately, max_tokens=384, start_on=\"human\", end_on=(\"human\", \"tool\"), ) # highlight-next-line return {\"llm_input_messages\": trimmed_messages} checkpointer = InMemorySaver() agent = create_react_agent( model, tools, # highlight-next-line pre_model_hook=pre_model_hook, checkpointer=checkpointer, ) ``` To learn more about using pre_model_hook for managing message history, see this how-to guide Read in tools { #read-short-term } LangGraph allows agent to access its short-term memory (state) inside the tools. ```python from typing import Annotated from langgraph.prebuilt import InjectedState, create_react_agent class CustomState(AgentState): # highlight-next-line user_id: str def get_user_info( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Look up user info.\"\"\" # highlight-next-line user_id = state[\"user_id\"] return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line state_schema=CustomState, ) agent.invoke({ \"messages\": \"look up user information\", # highlight-next-line \"user_id\": \"user_123\" }) ``` See the Context guide for more information. Write from tools { #write-short-term } To modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to"],"reference":"When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"How does ChatAnthropic facilitate the management of message history in LangGraph?","reference_contexts":["Long-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. Manage message history Long conversations can exceed the LLM's context window. Common solutions are: Summarization: Maintain a running summary of the conversation Trimming: Remove first or last N messages in the history This allows the agent to keep track of the conversation without exceeding the LLM's context window. To manage message history, specify pre_model_hook \u2014 a function (node) that will always run before calling the language model. Summarize message history To summarize message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with a prebuilt SummarizationNode: ```python from langchain_anthropic import ChatAnthropic from langmem.short_term import SummarizationNode from langchain_core.messages.utils import count_tokens_approximately from langgraph.prebuilt import create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.checkpoint.memory import InMemorySaver from typing import Any model = ChatAnthropic(model=\"claude-3-7-sonnet-latest\") summarization_node = SummarizationNode( # (1)! token_counter=count_tokens_approximately, model=model, max_tokens=384, max_summary_tokens=128, output_messages_key=\"llm_input_messages\", ) class State(AgentState): # NOTE: we're adding this key to keep track of previous summary information # to make sure we're not summarizing on every LLM call # highlight-next-line context: dict[str, Any] # (2)! checkpointer = InMemorySaver() # (3)! agent = create_react_agent( model=model, tools=tools, # highlight-next-line pre_model_hook=summarization_node, # (4)! # highlight-next-line state_schema=State, # (5)! checkpointer=checkpointer, ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The context key is added to the agent's state. The key contains book-keeping information for the summarization node. It is used to keep track of the last summary information and ensure that the agent doesn't summarize on every LLM call, which can be inefficient. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. The pre_model_hook is set to the SummarizationNode. This node will summarize the message history before sending it to the LLM. The summarization node will automatically handle the summarization process and update the agent's state with the new summary. You can replace this with a custom implementation if you prefer. Please see the [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent] API reference for more details. The state_schema is set to the State class, which is the custom state that contains an extra context key. Trim message history To trim message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with trim_messages function: ```python highlight-next-line from langchain_core.messages.utils import ( # highlight-next-line trim_messages, # highlight-next-line count_tokens_approximately highlight-next-line ) from langgraph.prebuilt import create_react_agent This function will be called every time before the node that calls LLM def pre_model_hook(state): trimmed_messages = trim_messages( state[\"messages\"], strategy=\"last\", token_counter=count_tokens_approximately, max_tokens=384, start_on=\"human\", end_on=(\"human\", \"tool\"), ) # highlight-next-line return {\"llm_input_messages\": trimmed_messages} checkpointer = InMemorySaver() agent = create_react_agent( model, tools, # highlight-next-line pre_model_hook=pre_model_hook, checkpointer=checkpointer, ) ``` To learn more about using pre_model_hook for managing message history, see this how-to guide Read in tools { #read-short-term } LangGraph allows agent to access its short-term memory (state) inside the tools. ```python from typing import Annotated from langgraph.prebuilt import InjectedState, create_react_agent class CustomState(AgentState): # highlight-next-line user_id: str def get_user_info( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Look up user info.\"\"\" # highlight-next-line user_id = state[\"user_id\"] return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line state_schema=CustomState, ) agent.invoke({ \"messages\": \"look up user information\", # highlight-next-line \"user_id\": \"user_123\" }) ``` See the Context guide for more information. Write from tools { #write-short-term } To modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to"],"reference":"ChatAnthropic, when used in LangGraph, facilitates the management of message history through the implementation of a pre_model_hook, which can summarize or trim messages to ensure that long conversations do not exceed the LLM's context window. This is achieved by using a SummarizationNode to maintain a running summary of the conversation or by employing a trim_messages function to remove excess messages. Additionally, the InMemorySaver checkpointer is utilized to store the agent's state in memory, allowing for persistence across invocations.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"What does sf refer to in the context of LangGraph?","reference_contexts":["search: boost: 2 tags: - agent hide: - tags Memory LangGraph supports two types of memory essential for building conversational agents: Short-term memory: Tracks the ongoing conversation by maintaining message history within a session. Long-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. "],"reference":"In the context of LangGraph, 'sf' refers to San Francisco, as indicated in the example where the agent is invoked with the message asking about the weather in 'sf'.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"Wht is LangGraph?","reference_contexts":["search: boost: 2 tags: - agent hide: - tags Memory LangGraph supports two types of memory essential for building conversational agents: Short-term memory: Tracks the ongoing conversation by maintaining message history within a session. Long-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. "],"reference":"LangGraph supports two types of memory essential for building conversational agents: Short-term memory, which tracks the ongoing conversation by maintaining message history within a session, and Long-term memory, which stores user-specific or application-level data across sessions.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"How does the system handle user information for user_123 in LangGraph?","reference_contexts":["subsequent tools or prompts. ```python from typing import Annotated from langchain_core.tools import InjectedToolCallId from langchain_core.runnables import RunnableConfig from langchain_core.messages import ToolMessage from langgraph.prebuilt import InjectedState, create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.types import Command class CustomState(AgentState): # highlight-next-line user_name: str def update_user_info( tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig ) -> Command: \"\"\"Look up and update user info.\"\"\" user_id = config[\"configurable\"].get(\"user_id\") name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\" # highlight-next-line return Command(update={ # highlight-next-line \"user_name\": name, # update the message history \"messages\": [ ToolMessage( \"Successfully looked up user information\", tool_call_id=tool_call_id ) ] }) def greet( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Use this to greet the user once you found their info.\"\"\" user_name = state[\"user_name\"] return f\"Hello {user_name}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[update_user_info, greet], # highlight-next-line state_schema=CustomState ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} ) ``` For more details, see how to update state from tools. Long-term memory Use long-term memory to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information. To use long-term memory, you need to: Configure a store to persist data across invocations. Use the [get_store][langgraph.config.get_store] function to access the store from within tools or prompts. Read { #read-long-term } ```python title=\"A tool the agent can use to look up user information\" from langchain_core.runnables import RunnableConfig from langgraph.config import get_store from langgraph.prebuilt import create_react_agent from langgraph.store.memory import InMemoryStore highlight-next-line store = InMemoryStore() # (1)! highlight-next-line store.put( # (2)! (\"users\",), # (3)! \"user_123\", # (4)! { \"name\": \"John Smith\", \"language\": \"English\", } # (5)! ) def get_user_info(config: RunnableConfig) -> str: \"\"\"Look up user info.\"\"\" # Same as that provided to create_react_agent # highlight-next-line store = get_store() # (6)! user_id = config[\"configurable\"].get(\"user_id\") # highlight-next-line user_info = store.get((\"users\",), user_id) # (7)! return str(user_info.value) if user_info else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line store=store # (8)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} ) ``` The InMemoryStore is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you. For this example, we write some sample data to the store using the put method. Please see the [BaseStore.put][langgraph.store.base.BaseStore.put] API reference for more details. The first argument is the namespace. This is used to group related data together. In this case, we are using the users namespace to group user data. A key within the namespace. This example uses a user ID for the key. The data that we want to store for the given user. The get_store function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created. The get method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a StoreValue object, which contains the value and metadata about the value. The store is passed to the agent. This enables the agent to access the store when running tools. You can also use the get_store function to access the store from anywhere in your code. Write { #write-long-term } ```python title=\"Example of a tool that updates user information\" from typing_extensions import TypedDict from langgraph.config import get_store from langgraph.prebuilt import create_react_agent from langgraph.store.memory import InMemoryStore store = InMemoryStore() # (1)! class UserInfo(TypedDict): # (2)! name: str def save_user_info(user_info: UserInfo, config: RunnableConfig) -> str: # (3)! \"\"\"Save user info.\"\"\" # Same as that provided to create_react_agent # highlight-next-line store = get_store() # (4)! user_id = config[\"configurable\"].get(\"user_id\") # highlight-next-line store.put((\"users\",), user_id, user_info) # (5)! return \"Successfully saved user info.\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[save_user_info], # highlight-next-line store=store ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} # (6)! ) You can access the store directly to get the value store.get((\"users\",), \"user_123\").value ``` The InMemoryStore is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you. The UserInfo class is a TypedDict that defines the structure of the user information. The LLM will use this to format the response according to the schema. The save_user_info function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information. The get_store function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created. The put method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store. The user_id is passed in the config. This is used to identify the user whose information is being updated."],"reference":"The system uses a tool called 'update_user_info' to look up and update user information. When the user_id is 'user_123', the name is set to 'John Smith'. The agent can greet the user by retrieving their name from the state, which is updated with the user information. Additionally, the InMemoryStore is utilized to store user data, allowing the agent to access and manage user-specific information across conversations.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"How does RunnableConfig facilitate user information management in LangGraph?","reference_contexts":["subsequent tools or prompts. ```python from typing import Annotated from langchain_core.tools import InjectedToolCallId from langchain_core.runnables import RunnableConfig from langchain_core.messages import ToolMessage from langgraph.prebuilt import InjectedState, create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.types import Command class CustomState(AgentState): # highlight-next-line user_name: str def update_user_info( tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig ) -> Command: \"\"\"Look up and update user info.\"\"\" user_id = config[\"configurable\"].get(\"user_id\") name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\" # highlight-next-line return Command(update={ # highlight-next-line \"user_name\": name, # update the message history \"messages\": [ ToolMessage( \"Successfully looked up user information\", tool_call_id=tool_call_id ) ] }) def greet( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Use this to greet the user once you found their info.\"\"\" user_name = state[\"user_name\"] return f\"Hello {user_name}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[update_user_info, greet], # highlight-next-line state_schema=CustomState ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} ) ``` For more details, see how to update state from tools. Long-term memory Use long-term memory to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information. To use long-term memory, you need to: Configure a store to persist data across invocations. Use the [get_store][langgraph.config.get_store] function to access the store from within tools or prompts. Read { #read-long-term } ```python title=\"A tool the agent can use to look up user information\" from langchain_core.runnables import RunnableConfig from langgraph.config import get_store from langgraph.prebuilt import create_react_agent from langgraph.store.memory import InMemoryStore highlight-next-line store = InMemoryStore() # (1)! highlight-next-line store.put( # (2)! (\"users\",), # (3)! \"user_123\", # (4)! { \"name\": \"John Smith\", \"language\": \"English\", } # (5)! ) def get_user_info(config: RunnableConfig) -> str: \"\"\"Look up user info.\"\"\" # Same as that provided to create_react_agent # highlight-next-line store = get_store() # (6)! user_id = config[\"configurable\"].get(\"user_id\") # highlight-next-line user_info = store.get((\"users\",), user_id) # (7)! return str(user_info.value) if user_info else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line store=store # (8)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} ) ``` The InMemoryStore is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you. For this example, we write some sample data to the store using the put method. Please see the [BaseStore.put][langgraph.store.base.BaseStore.put] API reference for more details. The first argument is the namespace. This is used to group related data together. In this case, we are using the users namespace to group user data. A key within the namespace. This example uses a user ID for the key. The data that we want to store for the given user. The get_store function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created. The get method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a StoreValue object, which contains the value and metadata about the value. The store is passed to the agent. This enables the agent to access the store when running tools. You can also use the get_store function to access the store from anywhere in your code. Write { #write-long-term } ```python title=\"Example of a tool that updates user information\" from typing_extensions import TypedDict from langgraph.config import get_store from langgraph.prebuilt import create_react_agent from langgraph.store.memory import InMemoryStore store = InMemoryStore() # (1)! class UserInfo(TypedDict): # (2)! name: str def save_user_info(user_info: UserInfo, config: RunnableConfig) -> str: # (3)! \"\"\"Save user info.\"\"\" # Same as that provided to create_react_agent # highlight-next-line store = get_store() # (4)! user_id = config[\"configurable\"].get(\"user_id\") # highlight-next-line store.put((\"users\",), user_id, user_info) # (5)! return \"Successfully saved user info.\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[save_user_info], # highlight-next-line store=store ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} # (6)! ) You can access the store directly to get the value store.get((\"users\",), \"user_123\").value ``` The InMemoryStore is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you. The UserInfo class is a TypedDict that defines the structure of the user information. The LLM will use this to format the response according to the schema. The save_user_info function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information. The get_store function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created. The put method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store. The user_id is passed in the config. This is used to identify the user whose information is being updated."],"reference":"RunnableConfig is utilized in LangGraph to manage user information by allowing tools to access and manipulate user-specific data. For instance, it can be used to look up user information or update it based on the provided configuration. The configuration typically includes a user ID, which is essential for retrieving or saving user data in a store, such as InMemoryStore. This enables the agent to maintain user context across interactions, enhancing the overall user experience.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"How does LangGraph implement semantic search for long-term memory?","reference_contexts":["Semantic search LangGraph also allows you to search for items in long-term memory by semantic similarity. Prebuilt memory tools LangMem is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the LangMem documentation for usage examples. Additional resources Memory in LangGraph"],"reference":"LangGraph allows you to search for items in long-term memory by semantic similarity, utilizing prebuilt memory tools from the LangMem library, which is maintained by LangChain.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"How does LangChain facilitate the management of long-term memories in agents, particularly in relation to semantic search capabilities?","reference_contexts":["Semantic search LangGraph also allows you to search for items in long-term memory by semantic similarity. Prebuilt memory tools LangMem is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the LangMem documentation for usage examples. Additional resources Memory in LangGraph"],"reference":"LangChain facilitates the management of long-term memories in agents through its prebuilt memory tools, specifically the LangMem library. This library offers tools for managing long-term memories, which can be utilized in conjunction with LangGraph's ability to search for items in long-term memory by semantic similarity.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"What is AWS Bedrock used for in LangGraph?","reference_contexts":["search: boost: 2 tags: - anthropic - openai - agent hide: - tags Models This page describes how to configure the chat model used by an agent. Tool calling support To enable tool-calling agents, the underlying LLM must support tool calling. Compatible models can be found in the LangChain integrations directory. Specifying a model by name You can configure an agent with a model name string: === \"OpenAI\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" agent = create_react_agent( # highlight-next-line model=\"openai:gpt-4.1\", # other parameters ) ``` === \"Anthropic\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" agent = create_react_agent( # highlight-next-line model=\"anthropic:claude-3-7-sonnet-latest\", # other parameters ) ``` === \"Azure\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\" os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\" os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\" agent = create_react_agent( # highlight-next-line model=\"azure_openai:gpt-4.1\", # other parameters ) ``` === \"Google Gemini\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"GOOGLE_API_KEY\"] = \"...\" agent = create_react_agent( # highlight-next-line model=\"google_genai:gemini-2.0-flash\", # other parameters ) ``` === \"AWS Bedrock\" ```python from langgraph.prebuilt import create_react_agent # Follow the steps here to configure your credentials: # https:\/\/docs.aws.amazon.com\/bedrock\/latest\/userguide\/getting-started.html agent = create_react_agent( # highlight-next-line model=\"bedrock_converse:anthropic.claude-3-5-sonnet-20240620-v1:0\", # other parameters ) ``` Using init_chat_model The init_chat_model utility simplifies model initialization with configurable parameters: === \"OpenAI\" ``` pip install -U \"langchain[openai]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" model = init_chat_model( \"openai:gpt-4.1\", temperature=0, # other parameters ) ``` === \"Anthropic\" ``` pip install -U \"langchain[anthropic]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" model = init_chat_model( \"anthropic:claude-3-5-sonnet-latest\", temperature=0, # other parameters ) ``` === \"Azure\" ``` pip install -U \"langchain[openai]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\" os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\" os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\" model = init_chat_model( \"azure_openai:gpt-4.1\", azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"], temperature=0, # other parameters ) ``` === \"Google Gemini\" ``` pip install -U \"langchain[google-genai]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"GOOGLE_API_KEY\"] = \"...\" model = init_chat_model( \"google_genai:gemini-2.0-flash\", temperature=0, # other parameters ) ``` === \"AWS Bedrock\" ``` pip install -U \"langchain[aws]\" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https:\/\/docs.aws.amazon.com\/bedrock\/latest\/userguide\/getting-started.html model = init_chat_model( \"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_provider=\"bedrock_converse\", temperature=0, # other parameters ) ``` Refer to the API reference for advanced options. Using provider-specific LLMs If a model provider is not available via init_chat_model, you can instantiate the provider's model class directly. The model must implement the BaseChatModel interface and support tool calling: ```python from langchain_anthropic import ChatAnthropic from langgraph.prebuilt import create_react_agent model = ChatAnthropic( model=\"claude-3-7-sonnet-latest\", temperature=0, max_tokens=2048 ) agent = create_react_agent( # highlight-next-line model=model, # other parameters ) ``` !!! note \"Illustrative example\" The example above uses `ChatAnthropic`, which is already supported by `init_chat_model`. This pattern is shown to illustrate how to manually instantiate a model not available through init_chat_model. Disable streaming To disable streaming of the individual LLM tokens, set disable_streaming=True when initializing the model: === \"init_chat_model\" ```python from langchain.chat_models import init_chat_model model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line disable_streaming=True ) ``` === \"ChatModel\" ```python from langchain_anthropic import ChatAnthropic model = ChatAnthropic( model=\"claude-3-7-sonnet-latest\", # highlight-next-line disable_streaming=True ) ``` Refer to the API reference for more information on disable_streaming"],"reference":"AWS Bedrock is used to configure agents in LangGraph by allowing the creation of a react agent with specific model parameters, such as 'bedrock_converse:anthropic.claude-3-5-sonnet-20240620-v1:0'.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"Can you explain how AWS Bedrock can be integrated into a system architecture using LangGraph?","reference_contexts":["search: boost: 2 tags: - anthropic - openai - agent hide: - tags Models This page describes how to configure the chat model used by an agent. Tool calling support To enable tool-calling agents, the underlying LLM must support tool calling. Compatible models can be found in the LangChain integrations directory. Specifying a model by name You can configure an agent with a model name string: === \"OpenAI\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" agent = create_react_agent( # highlight-next-line model=\"openai:gpt-4.1\", # other parameters ) ``` === \"Anthropic\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" agent = create_react_agent( # highlight-next-line model=\"anthropic:claude-3-7-sonnet-latest\", # other parameters ) ``` === \"Azure\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\" os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\" os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\" agent = create_react_agent( # highlight-next-line model=\"azure_openai:gpt-4.1\", # other parameters ) ``` === \"Google Gemini\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"GOOGLE_API_KEY\"] = \"...\" agent = create_react_agent( # highlight-next-line model=\"google_genai:gemini-2.0-flash\", # other parameters ) ``` === \"AWS Bedrock\" ```python from langgraph.prebuilt import create_react_agent # Follow the steps here to configure your credentials: # https:\/\/docs.aws.amazon.com\/bedrock\/latest\/userguide\/getting-started.html agent = create_react_agent( # highlight-next-line model=\"bedrock_converse:anthropic.claude-3-5-sonnet-20240620-v1:0\", # other parameters ) ``` Using init_chat_model The init_chat_model utility simplifies model initialization with configurable parameters: === \"OpenAI\" ``` pip install -U \"langchain[openai]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" model = init_chat_model( \"openai:gpt-4.1\", temperature=0, # other parameters ) ``` === \"Anthropic\" ``` pip install -U \"langchain[anthropic]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" model = init_chat_model( \"anthropic:claude-3-5-sonnet-latest\", temperature=0, # other parameters ) ``` === \"Azure\" ``` pip install -U \"langchain[openai]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\" os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\" os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\" model = init_chat_model( \"azure_openai:gpt-4.1\", azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"], temperature=0, # other parameters ) ``` === \"Google Gemini\" ``` pip install -U \"langchain[google-genai]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"GOOGLE_API_KEY\"] = \"...\" model = init_chat_model( \"google_genai:gemini-2.0-flash\", temperature=0, # other parameters ) ``` === \"AWS Bedrock\" ``` pip install -U \"langchain[aws]\" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https:\/\/docs.aws.amazon.com\/bedrock\/latest\/userguide\/getting-started.html model = init_chat_model( \"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_provider=\"bedrock_converse\", temperature=0, # other parameters ) ``` Refer to the API reference for advanced options. Using provider-specific LLMs If a model provider is not available via init_chat_model, you can instantiate the provider's model class directly. The model must implement the BaseChatModel interface and support tool calling: ```python from langchain_anthropic import ChatAnthropic from langgraph.prebuilt import create_react_agent model = ChatAnthropic( model=\"claude-3-7-sonnet-latest\", temperature=0, max_tokens=2048 ) agent = create_react_agent( # highlight-next-line model=model, # other parameters ) ``` !!! note \"Illustrative example\" The example above uses `ChatAnthropic`, which is already supported by `init_chat_model`. This pattern is shown to illustrate how to manually instantiate a model not available through init_chat_model. Disable streaming To disable streaming of the individual LLM tokens, set disable_streaming=True when initializing the model: === \"init_chat_model\" ```python from langchain.chat_models import init_chat_model model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line disable_streaming=True ) ``` === \"ChatModel\" ```python from langchain_anthropic import ChatAnthropic model = ChatAnthropic( model=\"claude-3-7-sonnet-latest\", # highlight-next-line disable_streaming=True ) ``` Refer to the API reference for more information on disable_streaming"],"reference":"AWS Bedrock can be integrated into a system architecture by following specific steps to configure your credentials. You can use the `create_react_agent` function from LangGraph to set up an agent with a model from AWS Bedrock. The model can be specified as `bedrock_converse:anthropic.claude-3-5-sonnet-20240620-v1:0`, along with other parameters. For detailed instructions on getting started, you can refer to the AWS documentation.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"How can I implement model fallbacks using openai:gpt-4.1-mini in LangGraph?","reference_contexts":["Adding model fallbacks You can add a fallback to a different model or a different LLM provider using model.with_fallbacks([...]): === \"init_chat_model\" ```python from langchain.chat_models import init_chat_model model_with_fallbacks = ( init_chat_model(\"anthropic:claude-3-5-haiku-latest\") # highlight-next-line .with_fallbacks([ init_chat_model(\"openai:gpt-4.1-mini\"), ]) ) ``` === \"ChatModel\" ```python from langchain_anthropic import ChatAnthropic from langchain_openai import ChatOpenAI model_with_fallbacks = ( ChatAnthropic(model=\"claude-3-5-haiku-latest\") # highlight-next-line .with_fallbacks([ ChatOpenAI(model=\"gpt-4.1-mini\"), ]) ) ``` See this guide for more information on model fallbacks. Additional resources Model integration directory Universal initialization with init_chat_model"],"reference":"To implement model fallbacks using openai:gpt-4.1-mini in LangGraph, you can use the method model.with_fallbacks([...]). For example, you can initialize a chat model with a fallback to openai:gpt-4.1-mini as follows: ```python from langchain.chat_models import init_chat_model model_with_fallbacks = ( init_chat_model(\"anthropic:claude-3-5-haiku-latest\") # highlight-next-line .with_fallbacks([ init_chat_model(\"openai:gpt-4.1-mini\"), ]) ) ``` This allows you to add a fallback to a different model or LLM provider.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"How can ChatAnthropic be used with model fallbacks?","reference_contexts":["Adding model fallbacks You can add a fallback to a different model or a different LLM provider using model.with_fallbacks([...]): === \"init_chat_model\" ```python from langchain.chat_models import init_chat_model model_with_fallbacks = ( init_chat_model(\"anthropic:claude-3-5-haiku-latest\") # highlight-next-line .with_fallbacks([ init_chat_model(\"openai:gpt-4.1-mini\"), ]) ) ``` === \"ChatModel\" ```python from langchain_anthropic import ChatAnthropic from langchain_openai import ChatOpenAI model_with_fallbacks = ( ChatAnthropic(model=\"claude-3-5-haiku-latest\") # highlight-next-line .with_fallbacks([ ChatOpenAI(model=\"gpt-4.1-mini\"), ]) ) ``` See this guide for more information on model fallbacks. Additional resources Model integration directory Universal initialization with init_chat_model"],"reference":"ChatAnthropic can be used with model fallbacks by initializing it with the desired model and then applying the .with_fallbacks() method to include other models. For example, you can create a model with fallbacks using the following code: `model_with_fallbacks = ( ChatAnthropic(model=\"claude-3-5-haiku-latest\") .with_fallbacks([ ChatOpenAI(model=\"gpt-4.1-mini\"), ]) )`.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"What is the role of Claude-3-7-sonnet-latest in creating an agent with LangGraph?","reference_contexts":["search: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent."],"reference":"Claude-3-7-sonnet-latest is specified as the model to be used when creating an agent with LangGraph, allowing the agent to perform tasks such as responding to user queries.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"what is the weather in sf?","reference_contexts":["search: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent."],"reference":"To get the weather for a given city like sf, you can create an agent using LangGraph's prebuilt components. For example, you can define a function called get_weather that returns a string indicating the weather, and then invoke the agent with a message asking about the weather in sf.","synthesizer_name":"single_hop_specifc_query_synthesizer"}
{"user_input":"How do you create an agent in LangGraph and configure its temperature parameter?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"To create an agent in LangGraph, you can use the `create_react_agent` function. For example, you can define a simple function to get the weather and then create the agent as follows: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], prompt=\"You are a helpful assistant\" ) ``` To configure the temperature parameter for the language model, you can use the `init_chat_model` function. Here\u2019s how you can set it: ```python from langchain.chat_models import init_chat_model model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", temperature=0 ) agent = create_react_agent( model=model, tools=[get_weather], ) ``` This way, you can create an agent and customize its behavior using the temperature parameter.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"What are the steps involved in configuring an LLM for use with LangGraph, and how does this relate to the creation of an agent?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"To configure an LLM for use with LangGraph, you first need to initialize the chat model with specific parameters, such as temperature, using the `init_chat_model` function. For example, you can set it up as follows: `model = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\", temperature=0)`. After configuring the LLM, you can create an agent using the `create_react_agent` function, where you provide the model and any tools the agent will use. This process is crucial for ensuring that the agent can effectively interact with users and perform tasks, such as retrieving weather information. The steps include defining the model, specifying tools, and optionally adding prompts to guide the agent's behavior. This configuration is essential for leveraging LangGraph's capabilities in building intelligent systems.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"What are the steps involved in creating an agent using LangGraph, and how can the temperature parameter be configured for the language model used by the agent?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"To create an agent using LangGraph, follow these steps: First, ensure you have the necessary prerequisites, including an Anthropic API key and the installation of LangGraph and LangChain. Then, you can create an agent using the `create_react_agent` function, where you define a tool (like a function to get weather) and provide a system prompt. After creating the agent, you can run it by invoking it with user messages. To configure the language model (LLM) with specific parameters such as temperature, you use the `init_chat_model` function, specifying the model and the desired temperature value. This allows you to control the randomness of the model's responses, enhancing the agent's performance based on your requirements.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"What steps are involved in creating an agent in LangGraph, and how can the temperature parameter be configured for the language model used by the agent?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"To create an agent in LangGraph, you need to follow these steps: First, ensure you have the necessary prerequisites, including an Anthropic API key and the installation of LangGraph and LangChain. Then, you can create an agent using the `create_react_agent` function, where you define a tool for the agent to use, provide a language model, and set a system prompt. For example, you can define a function to get the weather for a city and create an agent with that function as a tool. To configure the language model with specific parameters like temperature, you use the `init_chat_model` function, where you can specify the temperature value. This allows you to control the randomness of the model's responses. After configuring the model, you can create the agent with the specified model and tools.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"How do Python functions serve as tools in LangGraph, and what role does memory management play in enabling multi-turn conversations?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"In LangGraph, Python functions serve as tools by allowing users to define specific functionalities that agents can utilize. For example, a function like 'get_weather' can be created to fetch weather information, which the agent can invoke as needed. Memory management is crucial for enabling multi-turn conversations, as it allows the agent to maintain context across interactions. By using a checkpointer, such as 'InMemorySaver', the agent can store its state and retrieve it during subsequent invocations, ensuring that the conversation flows naturally and retains relevant information from previous exchanges.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"How do Python functions serve as tools in LangGraph, and what role does memory management play in enabling multi-turn conversations with agents?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"In LangGraph, Python functions serve as tools by allowing users to define specific functionalities that agents can utilize. For example, a function like 'get_weather' can be created to fetch weather information for a given city, which the agent can then invoke as part of its operations. Memory management is crucial for enabling multi-turn conversations with agents, as it allows the agent to maintain context across interactions. By using a checkpointer, such as 'InMemorySaver', the agent can store its state at each step, enabling it to remember previous messages and user inputs. This capability allows for a more coherent and contextually aware interaction, as the agent can reference past conversations when responding to new queries.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"How can Python functions be used as tools in LangGraph while managing memory for multi-turn conversations?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"In LangGraph, Python functions can be defined as tools for agents, allowing them to perform specific tasks. For example, a function like `get_weather` can be created to fetch weather information. To manage memory for multi-turn conversations, a checkpointer, such as `InMemorySaver`, can be provided when creating an agent. This enables the agent to store its state at every step, allowing it to remember previous interactions and maintain context across multiple invocations.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"How does LangGraph's weather retrieval tool utilize dynamic prompts to enhance user interaction?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"LangGraph's weather retrieval tool utilizes dynamic prompts by defining a function that generates a message list based on the agent's state and configuration. This allows the agent to address the user personally, incorporating runtime information such as the user's name. For instance, the dynamic prompt function constructs a system message that includes the user's name, enhancing the interaction by making it more personalized. This approach enables the agent to adapt its responses based on the context and user input, thereby improving the overall user experience.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"How does short-term memory in LangGraph facilitate the use of conversational agents during multi-turn conversations?","reference_contexts":["<1-hop>\n\nLong-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. Manage message history Long conversations can exceed the LLM's context window. Common solutions are: Summarization: Maintain a running summary of the conversation Trimming: Remove first or last N messages in the history This allows the agent to keep track of the conversation without exceeding the LLM's context window. To manage message history, specify pre_model_hook \u2014 a function (node) that will always run before calling the language model. Summarize message history To summarize message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with a prebuilt SummarizationNode: ```python from langchain_anthropic import ChatAnthropic from langmem.short_term import SummarizationNode from langchain_core.messages.utils import count_tokens_approximately from langgraph.prebuilt import create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.checkpoint.memory import InMemorySaver from typing import Any model = ChatAnthropic(model=\"claude-3-7-sonnet-latest\") summarization_node = SummarizationNode( # (1)! token_counter=count_tokens_approximately, model=model, max_tokens=384, max_summary_tokens=128, output_messages_key=\"llm_input_messages\", ) class State(AgentState): # NOTE: we're adding this key to keep track of previous summary information # to make sure we're not summarizing on every LLM call # highlight-next-line context: dict[str, Any] # (2)! checkpointer = InMemorySaver() # (3)! agent = create_react_agent( model=model, tools=tools, # highlight-next-line pre_model_hook=summarization_node, # (4)! # highlight-next-line state_schema=State, # (5)! checkpointer=checkpointer, ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The context key is added to the agent's state. The key contains book-keeping information for the summarization node. It is used to keep track of the last summary information and ensure that the agent doesn't summarize on every LLM call, which can be inefficient. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. The pre_model_hook is set to the SummarizationNode. This node will summarize the message history before sending it to the LLM. The summarization node will automatically handle the summarization process and update the agent's state with the new summary. You can replace this with a custom implementation if you prefer. Please see the [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent] API reference for more details. The state_schema is set to the State class, which is the custom state that contains an extra context key. Trim message history To trim message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with trim_messages function: ```python highlight-next-line from langchain_core.messages.utils import ( # highlight-next-line trim_messages, # highlight-next-line count_tokens_approximately highlight-next-line ) from langgraph.prebuilt import create_react_agent This function will be called every time before the node that calls LLM def pre_model_hook(state): trimmed_messages = trim_messages( state[\"messages\"], strategy=\"last\", token_counter=count_tokens_approximately, max_tokens=384, start_on=\"human\", end_on=(\"human\", \"tool\"), ) # highlight-next-line return {\"llm_input_messages\": trimmed_messages} checkpointer = InMemorySaver() agent = create_react_agent( model, tools, # highlight-next-line pre_model_hook=pre_model_hook, checkpointer=checkpointer, ) ``` To learn more about using pre_model_hook for managing message history, see this how-to guide Read in tools { #read-short-term } LangGraph allows agent to access its short-term memory (state) inside the tools. ```python from typing import Annotated from langgraph.prebuilt import InjectedState, create_react_agent class CustomState(AgentState): # highlight-next-line user_id: str def get_user_info( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Look up user info.\"\"\" # highlight-next-line user_id = state[\"user_id\"] return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line state_schema=CustomState, ) agent.invoke({ \"messages\": \"look up user information\", # highlight-next-line \"user_id\": \"user_123\" }) ``` See the Context guide for more information. Write from tools { #write-short-term } To modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to","<2-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags Memory LangGraph supports two types of memory essential for building conversational agents: Short-term memory: Tracks the ongoing conversation by maintaining message history within a session. Long-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. "],"reference":"Short-term memory in LangGraph, also known as thread-level memory, enables conversational agents to track ongoing conversations by maintaining message history within a session. To effectively utilize short-term memory, an agent must be created with a checkpointer that allows for the persistence of the agent's state. Additionally, a unique thread_id must be supplied in the configuration when running the agent, which identifies the conversation session. This setup allows the agent to continue conversations seamlessly, inferring context from previous messages, thus enhancing the interaction quality in multi-turn conversations.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"How does short-term memory in LangGraph help conversational agents track ongoing conversations and what is the role of the checkpointer in this process?","reference_contexts":["<1-hop>\n\nLong-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. Manage message history Long conversations can exceed the LLM's context window. Common solutions are: Summarization: Maintain a running summary of the conversation Trimming: Remove first or last N messages in the history This allows the agent to keep track of the conversation without exceeding the LLM's context window. To manage message history, specify pre_model_hook \u2014 a function (node) that will always run before calling the language model. Summarize message history To summarize message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with a prebuilt SummarizationNode: ```python from langchain_anthropic import ChatAnthropic from langmem.short_term import SummarizationNode from langchain_core.messages.utils import count_tokens_approximately from langgraph.prebuilt import create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.checkpoint.memory import InMemorySaver from typing import Any model = ChatAnthropic(model=\"claude-3-7-sonnet-latest\") summarization_node = SummarizationNode( # (1)! token_counter=count_tokens_approximately, model=model, max_tokens=384, max_summary_tokens=128, output_messages_key=\"llm_input_messages\", ) class State(AgentState): # NOTE: we're adding this key to keep track of previous summary information # to make sure we're not summarizing on every LLM call # highlight-next-line context: dict[str, Any] # (2)! checkpointer = InMemorySaver() # (3)! agent = create_react_agent( model=model, tools=tools, # highlight-next-line pre_model_hook=summarization_node, # (4)! # highlight-next-line state_schema=State, # (5)! checkpointer=checkpointer, ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The context key is added to the agent's state. The key contains book-keeping information for the summarization node. It is used to keep track of the last summary information and ensure that the agent doesn't summarize on every LLM call, which can be inefficient. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. The pre_model_hook is set to the SummarizationNode. This node will summarize the message history before sending it to the LLM. The summarization node will automatically handle the summarization process and update the agent's state with the new summary. You can replace this with a custom implementation if you prefer. Please see the [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent] API reference for more details. The state_schema is set to the State class, which is the custom state that contains an extra context key. Trim message history To trim message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with trim_messages function: ```python highlight-next-line from langchain_core.messages.utils import ( # highlight-next-line trim_messages, # highlight-next-line count_tokens_approximately highlight-next-line ) from langgraph.prebuilt import create_react_agent This function will be called every time before the node that calls LLM def pre_model_hook(state): trimmed_messages = trim_messages( state[\"messages\"], strategy=\"last\", token_counter=count_tokens_approximately, max_tokens=384, start_on=\"human\", end_on=(\"human\", \"tool\"), ) # highlight-next-line return {\"llm_input_messages\": trimmed_messages} checkpointer = InMemorySaver() agent = create_react_agent( model, tools, # highlight-next-line pre_model_hook=pre_model_hook, checkpointer=checkpointer, ) ``` To learn more about using pre_model_hook for managing message history, see this how-to guide Read in tools { #read-short-term } LangGraph allows agent to access its short-term memory (state) inside the tools. ```python from typing import Annotated from langgraph.prebuilt import InjectedState, create_react_agent class CustomState(AgentState): # highlight-next-line user_id: str def get_user_info( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Look up user info.\"\"\" # highlight-next-line user_id = state[\"user_id\"] return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line state_schema=CustomState, ) agent.invoke({ \"messages\": \"look up user information\", # highlight-next-line \"user_id\": \"user_123\" }) ``` See the Context guide for more information. Write from tools { #write-short-term } To modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to","<2-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags Memory LangGraph supports two types of memory essential for building conversational agents: Short-term memory: Tracks the ongoing conversation by maintaining message history within a session. Long-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. "],"reference":"Short-term memory in LangGraph, also known as thread-level memory, enables conversational agents to track ongoing conversations by maintaining message history within a session. To effectively use short-term memory, a checkpointer must be provided when creating the agent. The checkpointer allows for the persistence of the agent's state across invocations, ensuring that the conversation can continue seamlessly. This is achieved by supplying a unique thread_id in the configuration, which identifies the conversation session and allows the agent to infer context from previous messages.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"What are the differences between long-term memory and short-term memory in LangGraph, and how do they relate to user-specific data storage?","reference_contexts":["<1-hop>\n\nLong-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. Manage message history Long conversations can exceed the LLM's context window. Common solutions are: Summarization: Maintain a running summary of the conversation Trimming: Remove first or last N messages in the history This allows the agent to keep track of the conversation without exceeding the LLM's context window. To manage message history, specify pre_model_hook \u2014 a function (node) that will always run before calling the language model. Summarize message history To summarize message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with a prebuilt SummarizationNode: ```python from langchain_anthropic import ChatAnthropic from langmem.short_term import SummarizationNode from langchain_core.messages.utils import count_tokens_approximately from langgraph.prebuilt import create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.checkpoint.memory import InMemorySaver from typing import Any model = ChatAnthropic(model=\"claude-3-7-sonnet-latest\") summarization_node = SummarizationNode( # (1)! token_counter=count_tokens_approximately, model=model, max_tokens=384, max_summary_tokens=128, output_messages_key=\"llm_input_messages\", ) class State(AgentState): # NOTE: we're adding this key to keep track of previous summary information # to make sure we're not summarizing on every LLM call # highlight-next-line context: dict[str, Any] # (2)! checkpointer = InMemorySaver() # (3)! agent = create_react_agent( model=model, tools=tools, # highlight-next-line pre_model_hook=summarization_node, # (4)! # highlight-next-line state_schema=State, # (5)! checkpointer=checkpointer, ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The context key is added to the agent's state. The key contains book-keeping information for the summarization node. It is used to keep track of the last summary information and ensure that the agent doesn't summarize on every LLM call, which can be inefficient. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. The pre_model_hook is set to the SummarizationNode. This node will summarize the message history before sending it to the LLM. The summarization node will automatically handle the summarization process and update the agent's state with the new summary. You can replace this with a custom implementation if you prefer. Please see the [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent] API reference for more details. The state_schema is set to the State class, which is the custom state that contains an extra context key. Trim message history To trim message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with trim_messages function: ```python highlight-next-line from langchain_core.messages.utils import ( # highlight-next-line trim_messages, # highlight-next-line count_tokens_approximately highlight-next-line ) from langgraph.prebuilt import create_react_agent This function will be called every time before the node that calls LLM def pre_model_hook(state): trimmed_messages = trim_messages( state[\"messages\"], strategy=\"last\", token_counter=count_tokens_approximately, max_tokens=384, start_on=\"human\", end_on=(\"human\", \"tool\"), ) # highlight-next-line return {\"llm_input_messages\": trimmed_messages} checkpointer = InMemorySaver() agent = create_react_agent( model, tools, # highlight-next-line pre_model_hook=pre_model_hook, checkpointer=checkpointer, ) ``` To learn more about using pre_model_hook for managing message history, see this how-to guide Read in tools { #read-short-term } LangGraph allows agent to access its short-term memory (state) inside the tools. ```python from typing import Annotated from langgraph.prebuilt import InjectedState, create_react_agent class CustomState(AgentState): # highlight-next-line user_id: str def get_user_info( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Look up user info.\"\"\" # highlight-next-line user_id = state[\"user_id\"] return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line state_schema=CustomState, ) agent.invoke({ \"messages\": \"look up user information\", # highlight-next-line \"user_id\": \"user_123\" }) ``` See the Context guide for more information. Write from tools { #write-short-term } To modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to","<2-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags Memory LangGraph supports two types of memory essential for building conversational agents: Short-term memory: Tracks the ongoing conversation by maintaining message history within a session. Long-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. ","<3-hop>\n\nsubsequent tools or prompts. ```python from typing import Annotated from langchain_core.tools import InjectedToolCallId from langchain_core.runnables import RunnableConfig from langchain_core.messages import ToolMessage from langgraph.prebuilt import InjectedState, create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.types import Command class CustomState(AgentState): # highlight-next-line user_name: str def update_user_info( tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig ) -> Command: \"\"\"Look up and update user info.\"\"\" user_id = config[\"configurable\"].get(\"user_id\") name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\" # highlight-next-line return Command(update={ # highlight-next-line \"user_name\": name, # update the message history \"messages\": [ ToolMessage( \"Successfully looked up user information\", tool_call_id=tool_call_id ) ] }) def greet( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Use this to greet the user once you found their info.\"\"\" user_name = state[\"user_name\"] return f\"Hello {user_name}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[update_user_info, greet], # highlight-next-line state_schema=CustomState ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} ) ``` For more details, see how to update state from tools. Long-term memory Use long-term memory to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information. To use long-term memory, you need to: Configure a store to persist data across invocations. Use the [get_store][langgraph.config.get_store] function to access the store from within tools or prompts. Read { #read-long-term } ```python title=\"A tool the agent can use to look up user information\" from langchain_core.runnables import RunnableConfig from langgraph.config import get_store from langgraph.prebuilt import create_react_agent from langgraph.store.memory import InMemoryStore highlight-next-line store = InMemoryStore() # (1)! highlight-next-line store.put( # (2)! (\"users\",), # (3)! \"user_123\", # (4)! { \"name\": \"John Smith\", \"language\": \"English\", } # (5)! ) def get_user_info(config: RunnableConfig) -> str: \"\"\"Look up user info.\"\"\" # Same as that provided to create_react_agent # highlight-next-line store = get_store() # (6)! user_id = config[\"configurable\"].get(\"user_id\") # highlight-next-line user_info = store.get((\"users\",), user_id) # (7)! return str(user_info.value) if user_info else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line store=store # (8)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} ) ``` The InMemoryStore is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you. For this example, we write some sample data to the store using the put method. Please see the [BaseStore.put][langgraph.store.base.BaseStore.put] API reference for more details. The first argument is the namespace. This is used to group related data together. In this case, we are using the users namespace to group user data. A key within the namespace. This example uses a user ID for the key. The data that we want to store for the given user. The get_store function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created. The get method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a StoreValue object, which contains the value and metadata about the value. The store is passed to the agent. This enables the agent to access the store when running tools. You can also use the get_store function to access the store from anywhere in your code. Write { #write-long-term } ```python title=\"Example of a tool that updates user information\" from typing_extensions import TypedDict from langgraph.config import get_store from langgraph.prebuilt import create_react_agent from langgraph.store.memory import InMemoryStore store = InMemoryStore() # (1)! class UserInfo(TypedDict): # (2)! name: str def save_user_info(user_info: UserInfo, config: RunnableConfig) -> str: # (3)! \"\"\"Save user info.\"\"\" # Same as that provided to create_react_agent # highlight-next-line store = get_store() # (4)! user_id = config[\"configurable\"].get(\"user_id\") # highlight-next-line store.put((\"users\",), user_id, user_info) # (5)! return \"Successfully saved user info.\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[save_user_info], # highlight-next-line store=store ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} # (6)! ) You can access the store directly to get the value store.get((\"users\",), \"user_123\").value ``` The InMemoryStore is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you. The UserInfo class is a TypedDict that defines the structure of the user information. The LLM will use this to format the response according to the schema. The save_user_info function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information. The get_store function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created. The put method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store. The user_id is passed in the config. This is used to identify the user whose information is being updated.","<4-hop>\n\nSemantic search LangGraph also allows you to search for items in long-term memory by semantic similarity. Prebuilt memory tools LangMem is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the LangMem documentation for usage examples. Additional resources Memory in LangGraph"],"reference":"In LangGraph, long-term memory is designed to store user-specific or application-level data across sessions, also referred to as cross-thread memory. This allows for the retention of information beyond a single conversation. In contrast, short-term memory, also known as thread-level memory, tracks ongoing conversations by maintaining message history within a session. Both memory types are essential for building conversational agents, with long-term memory focusing on persistent data storage and short-term memory managing immediate conversational context.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"How does LangGraph manage user information across sessions using long-term memory and what role does the InMemoryStore play in this process?","reference_contexts":["<1-hop>\n\nLong-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. Manage message history Long conversations can exceed the LLM's context window. Common solutions are: Summarization: Maintain a running summary of the conversation Trimming: Remove first or last N messages in the history This allows the agent to keep track of the conversation without exceeding the LLM's context window. To manage message history, specify pre_model_hook \u2014 a function (node) that will always run before calling the language model. Summarize message history To summarize message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with a prebuilt SummarizationNode: ```python from langchain_anthropic import ChatAnthropic from langmem.short_term import SummarizationNode from langchain_core.messages.utils import count_tokens_approximately from langgraph.prebuilt import create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.checkpoint.memory import InMemorySaver from typing import Any model = ChatAnthropic(model=\"claude-3-7-sonnet-latest\") summarization_node = SummarizationNode( # (1)! token_counter=count_tokens_approximately, model=model, max_tokens=384, max_summary_tokens=128, output_messages_key=\"llm_input_messages\", ) class State(AgentState): # NOTE: we're adding this key to keep track of previous summary information # to make sure we're not summarizing on every LLM call # highlight-next-line context: dict[str, Any] # (2)! checkpointer = InMemorySaver() # (3)! agent = create_react_agent( model=model, tools=tools, # highlight-next-line pre_model_hook=summarization_node, # (4)! # highlight-next-line state_schema=State, # (5)! checkpointer=checkpointer, ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The context key is added to the agent's state. The key contains book-keeping information for the summarization node. It is used to keep track of the last summary information and ensure that the agent doesn't summarize on every LLM call, which can be inefficient. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. The pre_model_hook is set to the SummarizationNode. This node will summarize the message history before sending it to the LLM. The summarization node will automatically handle the summarization process and update the agent's state with the new summary. You can replace this with a custom implementation if you prefer. Please see the [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent] API reference for more details. The state_schema is set to the State class, which is the custom state that contains an extra context key. Trim message history To trim message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with trim_messages function: ```python highlight-next-line from langchain_core.messages.utils import ( # highlight-next-line trim_messages, # highlight-next-line count_tokens_approximately highlight-next-line ) from langgraph.prebuilt import create_react_agent This function will be called every time before the node that calls LLM def pre_model_hook(state): trimmed_messages = trim_messages( state[\"messages\"], strategy=\"last\", token_counter=count_tokens_approximately, max_tokens=384, start_on=\"human\", end_on=(\"human\", \"tool\"), ) # highlight-next-line return {\"llm_input_messages\": trimmed_messages} checkpointer = InMemorySaver() agent = create_react_agent( model, tools, # highlight-next-line pre_model_hook=pre_model_hook, checkpointer=checkpointer, ) ``` To learn more about using pre_model_hook for managing message history, see this how-to guide Read in tools { #read-short-term } LangGraph allows agent to access its short-term memory (state) inside the tools. ```python from typing import Annotated from langgraph.prebuilt import InjectedState, create_react_agent class CustomState(AgentState): # highlight-next-line user_id: str def get_user_info( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Look up user info.\"\"\" # highlight-next-line user_id = state[\"user_id\"] return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line state_schema=CustomState, ) agent.invoke({ \"messages\": \"look up user information\", # highlight-next-line \"user_id\": \"user_123\" }) ``` See the Context guide for more information. Write from tools { #write-short-term } To modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to","<2-hop>\n\nsubsequent tools or prompts. ```python from typing import Annotated from langchain_core.tools import InjectedToolCallId from langchain_core.runnables import RunnableConfig from langchain_core.messages import ToolMessage from langgraph.prebuilt import InjectedState, create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.types import Command class CustomState(AgentState): # highlight-next-line user_name: str def update_user_info( tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig ) -> Command: \"\"\"Look up and update user info.\"\"\" user_id = config[\"configurable\"].get(\"user_id\") name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\" # highlight-next-line return Command(update={ # highlight-next-line \"user_name\": name, # update the message history \"messages\": [ ToolMessage( \"Successfully looked up user information\", tool_call_id=tool_call_id ) ] }) def greet( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Use this to greet the user once you found their info.\"\"\" user_name = state[\"user_name\"] return f\"Hello {user_name}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[update_user_info, greet], # highlight-next-line state_schema=CustomState ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} ) ``` For more details, see how to update state from tools. Long-term memory Use long-term memory to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information. To use long-term memory, you need to: Configure a store to persist data across invocations. Use the [get_store][langgraph.config.get_store] function to access the store from within tools or prompts. Read { #read-long-term } ```python title=\"A tool the agent can use to look up user information\" from langchain_core.runnables import RunnableConfig from langgraph.config import get_store from langgraph.prebuilt import create_react_agent from langgraph.store.memory import InMemoryStore highlight-next-line store = InMemoryStore() # (1)! highlight-next-line store.put( # (2)! (\"users\",), # (3)! \"user_123\", # (4)! { \"name\": \"John Smith\", \"language\": \"English\", } # (5)! ) def get_user_info(config: RunnableConfig) -> str: \"\"\"Look up user info.\"\"\" # Same as that provided to create_react_agent # highlight-next-line store = get_store() # (6)! user_id = config[\"configurable\"].get(\"user_id\") # highlight-next-line user_info = store.get((\"users\",), user_id) # (7)! return str(user_info.value) if user_info else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line store=store # (8)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} ) ``` The InMemoryStore is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you. For this example, we write some sample data to the store using the put method. Please see the [BaseStore.put][langgraph.store.base.BaseStore.put] API reference for more details. The first argument is the namespace. This is used to group related data together. In this case, we are using the users namespace to group user data. A key within the namespace. This example uses a user ID for the key. The data that we want to store for the given user. The get_store function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created. The get method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a StoreValue object, which contains the value and metadata about the value. The store is passed to the agent. This enables the agent to access the store when running tools. You can also use the get_store function to access the store from anywhere in your code. Write { #write-long-term } ```python title=\"Example of a tool that updates user information\" from typing_extensions import TypedDict from langgraph.config import get_store from langgraph.prebuilt import create_react_agent from langgraph.store.memory import InMemoryStore store = InMemoryStore() # (1)! class UserInfo(TypedDict): # (2)! name: str def save_user_info(user_info: UserInfo, config: RunnableConfig) -> str: # (3)! \"\"\"Save user info.\"\"\" # Same as that provided to create_react_agent # highlight-next-line store = get_store() # (4)! user_id = config[\"configurable\"].get(\"user_id\") # highlight-next-line store.put((\"users\",), user_id, user_info) # (5)! return \"Successfully saved user info.\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[save_user_info], # highlight-next-line store=store ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} # (6)! ) You can access the store directly to get the value store.get((\"users\",), \"user_123\").value ``` The InMemoryStore is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you. The UserInfo class is a TypedDict that defines the structure of the user information. The LLM will use this to format the response according to the schema. The save_user_info function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information. The get_store function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created. The put method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store. The user_id is passed in the config. This is used to identify the user whose information is being updated.","<3-hop>\n\nSemantic search LangGraph also allows you to search for items in long-term memory by semantic similarity. Prebuilt memory tools LangMem is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the LangMem documentation for usage examples. Additional resources Memory in LangGraph"],"reference":"LangGraph manages user information across sessions using long-term memory, which stores user-specific or application-level data persistently. The InMemoryStore is a key component in this process, as it allows the agent to store and retrieve user information efficiently. When a user interacts with the agent, their data can be saved in the InMemoryStore, enabling the agent to access this information in future sessions. This is particularly useful for applications like chatbots, where remembering user preferences enhances the user experience.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"What are the steps to configure an LLM for use with LangGraph, and how does this configuration integrate with the creation of an agent?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"To configure an LLM for use with LangGraph, you first need to initialize the chat model with specific parameters, such as temperature, using the `init_chat_model` function. For example, you can do this by importing the necessary modules and calling the function as follows: `model = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\", temperature=0)`. After configuring the LLM, you can create an agent using the `create_react_agent` function, where you pass the configured model and any tools you want the agent to use. This integration allows the agent to leverage the configured LLM for its operations, ensuring that it behaves according to the specified parameters and can utilize the defined tools effectively.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"How does memory management enhance the functionality of a language model when using a system prompt in LangGraph?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"Memory management enhances the functionality of a language model in LangGraph by allowing multi-turn conversations through the use of a checkpointer. This enables the agent to store its state at every step, facilitating short-term memory and human-in-the-loop capabilities. When a system prompt is provided, it instructs the language model on how to behave, and with memory management, the agent can maintain context across multiple interactions, improving the overall user experience.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"How does the system prompt for a language model influence memory management in LangGraph's agent creation process?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"The system prompt for a language model in LangGraph plays a crucial role in shaping the agent's behavior and interaction with users. When creating an agent, the prompt can be defined as either static or dynamic. A static prompt provides fixed instructions, while a dynamic prompt can adapt based on the agent's state and configuration. This adaptability is essential for effective memory management, as it allows the agent to maintain context over multiple interactions. For instance, when memory is enabled through a checkpointer, the agent can store its state at each step, allowing it to recall previous messages and user inputs in future conversations. This integration of the system prompt with memory management ensures that the agent can engage in coherent multi-turn dialogues, enhancing the overall user experience.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"How do you configure an LLM in LangGraph to work with a custom prompt and ensure it can handle dynamic inputs?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"To configure an LLM in LangGraph, you can use the `init_chat_model` function to set specific parameters like temperature. For example, you can initialize the model with `model = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\", temperature=0)`. Then, to create an agent that utilizes this model, you can use `create_react_agent`, passing the model and any tools you want the agent to use. For dynamic prompts, you can define a function that returns a message list based on the agent's state and configuration. This allows the agent to include runtime information, such as user-specific data, in its responses.","synthesizer_name":"multi_hop_abstract_query_synthesizer"}
{"user_input":"How can LangGraph integrate with OpenAI models and handle tool errors effectively in a large-scale system design?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - anthropic - openai - agent hide: - tags Models This page describes how to configure the chat model used by an agent. Tool calling support To enable tool-calling agents, the underlying LLM must support tool calling. Compatible models can be found in the LangChain integrations directory. Specifying a model by name You can configure an agent with a model name string: === \"OpenAI\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" agent = create_react_agent( # highlight-next-line model=\"openai:gpt-4.1\", # other parameters ) ``` === \"Anthropic\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" agent = create_react_agent( # highlight-next-line model=\"anthropic:claude-3-7-sonnet-latest\", # other parameters ) ``` === \"Azure\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\" os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\" os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\" agent = create_react_agent( # highlight-next-line model=\"azure_openai:gpt-4.1\", # other parameters ) ``` === \"Google Gemini\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"GOOGLE_API_KEY\"] = \"...\" agent = create_react_agent( # highlight-next-line model=\"google_genai:gemini-2.0-flash\", # other parameters ) ``` === \"AWS Bedrock\" ```python from langgraph.prebuilt import create_react_agent # Follow the steps here to configure your credentials: # https:\/\/docs.aws.amazon.com\/bedrock\/latest\/userguide\/getting-started.html agent = create_react_agent( # highlight-next-line model=\"bedrock_converse:anthropic.claude-3-5-sonnet-20240620-v1:0\", # other parameters ) ``` Using init_chat_model The init_chat_model utility simplifies model initialization with configurable parameters: === \"OpenAI\" ``` pip install -U \"langchain[openai]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" model = init_chat_model( \"openai:gpt-4.1\", temperature=0, # other parameters ) ``` === \"Anthropic\" ``` pip install -U \"langchain[anthropic]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" model = init_chat_model( \"anthropic:claude-3-5-sonnet-latest\", temperature=0, # other parameters ) ``` === \"Azure\" ``` pip install -U \"langchain[openai]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\" os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\" os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\" model = init_chat_model( \"azure_openai:gpt-4.1\", azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"], temperature=0, # other parameters ) ``` === \"Google Gemini\" ``` pip install -U \"langchain[google-genai]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"GOOGLE_API_KEY\"] = \"...\" model = init_chat_model( \"google_genai:gemini-2.0-flash\", temperature=0, # other parameters ) ``` === \"AWS Bedrock\" ``` pip install -U \"langchain[aws]\" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https:\/\/docs.aws.amazon.com\/bedrock\/latest\/userguide\/getting-started.html model = init_chat_model( \"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_provider=\"bedrock_converse\", temperature=0, # other parameters ) ``` Refer to the API reference for advanced options. Using provider-specific LLMs If a model provider is not available via init_chat_model, you can instantiate the provider's model class directly. The model must implement the BaseChatModel interface and support tool calling: ```python from langchain_anthropic import ChatAnthropic from langgraph.prebuilt import create_react_agent model = ChatAnthropic( model=\"claude-3-7-sonnet-latest\", temperature=0, max_tokens=2048 ) agent = create_react_agent( # highlight-next-line model=model, # other parameters ) ``` !!! note \"Illustrative example\" The example above uses `ChatAnthropic`, which is already supported by `init_chat_model`. This pattern is shown to illustrate how to manually instantiate a model not available through init_chat_model. Disable streaming To disable streaming of the individual LLM tokens, set disable_streaming=True when initializing the model: === \"init_chat_model\" ```python from langchain.chat_models import init_chat_model model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line disable_streaming=True ) ``` === \"ChatModel\" ```python from langchain_anthropic import ChatAnthropic model = ChatAnthropic( model=\"claude-3-7-sonnet-latest\", # highlight-next-line disable_streaming=True ) ``` Refer to the API reference for more information on disable_streaming","<2-hop>\n\nHandle tool errors By default, the agent will catch all exceptions raised during tool calls and will pass those as tool messages to the LLM. To control how the errors are handled, you can use the prebuilt [ToolNode][langgraph.prebuilt.tool_node.ToolNode] \u2014 the node that executes tools inside create_react_agent \u2014 via its handle_tool_errors parameter: === \"Enable error handling (default)\" ```python from langgraph.prebuilt import create_react_agent def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" if a == 42: raise ValueError(\"The ultimate error\") return a * b # Run with error handling (default) agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[multiply] ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]} ) ``` === \"Disable error handling\" ```python from langgraph.prebuilt import create_react_agent, ToolNode def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" if a == 42: raise ValueError(\"The ultimate error\") return a * b # highlight-next-line tool_node = ToolNode( [multiply], # highlight-next-line handle_tool_errors=False # (1)! ) agent_no_error_handling = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=tool_node ) agent_no_error_handling.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]} ) ``` 1. This disables error handling (enabled by default). See all available strategies in the [API reference][langgraph.prebuilt.tool_node.ToolNode]. === \"Custom error handling\" ```python from langgraph.prebuilt import create_react_agent, ToolNode def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" if a == 42: raise ValueError(\"The ultimate error\") return a * b # highlight-next-line tool_node = ToolNode( [multiply], # highlight-next-line handle_tool_errors=( \"Can't use 42 as a first operand, you must switch operands!\" # (1)! ) ) agent_custom_error_handling = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=tool_node ) agent_custom_error_handling.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]} ) ``` 1. This provides a custom message to send to the LLM in case of an exception. See all available strategies in the [API reference][langgraph.prebuilt.tool_node.ToolNode]. See [API reference][langgraph.prebuilt.tool_node.ToolNode] for more information on different tool error handling options. Working with memory LangGraph allows access to short-term and long-term memory from tools. See Memory guide for more information on: how to read from and write to short-term memory how to read from and write to long-term memory Prebuilt tools You can use prebuilt tools from model providers by passing a dictionary with tool specs to the tools parameter of create_react_agent. For example, to use the web_search_preview tool from OpenAI: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"openai:gpt-4o-mini\", tools=[{\"type\": \"web_search_preview\"}] ) response = agent.invoke( {\"messages\": [\"What was a positive news story from today?\"]} ) ``` Additionally, LangChain supports a wide range of prebuilt tool integrations for interacting with APIs, databases, file systems, web data, and more. These tools extend the functionality of agents and enable rapid development. You can browse the full list of available integrations in the LangChain integrations directory. Some commonly used tool categories include: Search: Bing, SerpAPI, Tavily Code interpreters: Python REPL, Node.js REPL Databases: SQL, MongoDB, Redis Web data: Web scraping and browsing APIs: OpenWeatherMap, NewsAPI, and others These integrations can be configured and added to your agents using the same tools parameter shown in the examples above."],"reference":"LangGraph can integrate with OpenAI models by configuring agents with specific model names, such as 'openai:gpt-4.1', using the create_react_agent function. This involves setting the OpenAI API key in the environment and specifying the model in the agent configuration. Additionally, LangGraph allows for error handling during tool calls. By default, the agent catches exceptions raised during tool calls and passes them as tool messages to the LLM. Developers can customize this behavior using the ToolNode's handle_tool_errors parameter, enabling them to either disable error handling or provide custom error messages. This flexibility is crucial for ensuring robust interactions with OpenAI models in large-scale system designs.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How does the LangGraph agent utilize the human-in-the-loop feature when booking a stay at the McKittrick hotel, and what is the process for resuming the agent after human input is received?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - human-in-the-loop - hil - agent hide: - tags Human-in-the-loop To review, edit and approve tool calls in an agent you can use LangGraph's built-in Human-In-the-Loop (HIL) features, specifically the [interrupt()][langgraph.types.interrupt] primitive. LangGraph allows you to pause execution indefinitely \u2014 for minutes, hours, or even days\u2014until human input is received. This is possible because the agent state is checkpointed into a database, which allows the system to persist execution context and later resume the workflow, continuing from where it left off. For a deeper dive into the human-in-the-loop concept, see the concept guide. Review tool calls To add a human approval step to a tool: Use interrupt() in the tool to pause execution. Resume with a Command(resume=...) to continue based on human input. ```python from langgraph.checkpoint.memory import InMemorySaver from langgraph.types import interrupt from langgraph.prebuilt import create_react_agent An example of a sensitive tool that requires human review \/ approval def book_hotel(hotel_name: str): \"\"\"Book a hotel\"\"\" # highlight-next-line response = interrupt( # (1)! f\"Trying to call book_hotel with args {{'hotel_name': {hotel_name}}}. \" \"Please approve or suggest edits.\" ) if response[\"type\"] == \"accept\": pass elif response[\"type\"] == \"edit\": hotel_name = response[\"args\"][\"hotel_name\"] else: raise ValueError(f\"Unknown response type: {response['type']}\") return f\"Successfully booked a stay at {hotel_name}.\" highlight-next-line checkpointer = InMemorySaver() # (2)! agent = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", tools=[book_hotel], # highlight-next-line checkpointer=checkpointer, # (3)! ) ``` The [interrupt function][langgraph.types.interrupt] pauses the agent graph at a specific node. In this case, we call interrupt() at the beginning of the tool function, which pauses the graph at the node that executes the tool. The information inside interrupt() (e.g., tool calls) can be presented to a human, and the graph can be resumed with the user input (tool call approval, edit or feedback). The InMemorySaver is used to store the agent state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. In this example, we use InMemorySaver to store the agent state in memory. In a production application, the agent state will be stored in a database. Initialize the agent with the checkpointer. Run the agent with the stream() method, passing the config object to specify the thread ID. This allows the agent to resume the same conversation on future invocations. ```python config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" } } for chunk in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]}, # highlight-next-line config ): print(chunk) print(\"\\n\") ``` You should see that the agent runs until it reaches the interrupt() call, at which point it pauses and waits for human input. Resume the agent with a Command(resume=...) to continue based on human input. ```python from langgraph.types import Command for chunk in agent.stream( # highlight-next-line Command(resume={\"type\": \"accept\"}), # (1)! # Command(resume={\"type\": \"edit\", \"args\": {\"hotel_name\": \"McKittrick Hotel\"}}), config ): print(chunk) print(\"\\n\") ``` The [interrupt function][langgraph.types.interrupt] is used in conjunction with the [Command][langgraph.types.Command] object to resume the graph with a value provided by the human. ","<2-hop>\n\nRun the agent with the stream() method, passing the config object to specify the thread ID. This allows the agent to resume the same conversation on future invocations. ```python config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" } } for chunk in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]}, # highlight-next-line config ): print(chunk) print(\"\\n\") ``` You should see that the agent runs until it reaches the interrupt() call, at which point it pauses and waits for human input. Resume the agent with a Command(resume=...) to continue based on human input. ```python from langgraph.types import Command for chunk in agent.stream( # highlight-next-line Command(resume={\"type\": \"accept\"}), # (1)! # Command(resume={\"type\": \"edit\", \"args\": {\"hotel_name\": \"McKittrick Hotel\"}}), config ): print(chunk) print(\"\\n\") ``` The [interrupt function][langgraph.types.interrupt] is used in conjunction with the [Command][langgraph.types.Command] object to resume the graph with a value provided by the human. Using with Agent Inbox You can create a wrapper to add interrupts to any tool. The example below provides a reference implementation compatible with Agent Inbox UI and Agent Chat UI. ```python title=\"Wrapper that adds human-in-the-loop to any tool\" from typing import Callable from langchain_core.tools import BaseTool, tool as create_tool from langchain_core.runnables import RunnableConfig from langgraph.types import interrupt from langgraph.prebuilt.interrupt import HumanInterruptConfig, HumanInterrupt def add_human_in_the_loop( tool: Callable | BaseTool, *, interrupt_config: HumanInterruptConfig = None, ) -> BaseTool: \"\"\"Wrap a tool to support human-in-the-loop review.\"\"\" if not isinstance(tool, BaseTool): tool = create_tool(tool) if interrupt_config is None: interrupt_config = { \"allow_accept\": True, \"allow_edit\": True, \"allow_respond\": True, } @create_tool( # (1)! tool.name, description=tool.description, args_schema=tool.args_schema ) def call_tool_with_interrupt(config: RunnableConfig, **tool_input): request: HumanInterrupt = { \"action_request\": { \"action\": tool.name, \"args\": tool_input }, \"config\": interrupt_config, \"description\": \"Please review the tool call\" } # highlight-next-line response = interrupt([request])[0] # (2)! # approve the tool call if response[\"type\"] == \"accept\": tool_response = tool.invoke(tool_input, config) # update tool call args elif response[\"type\"] == \"edit\": tool_input = response[\"args\"][\"args\"] tool_response = tool.invoke(tool_input, config) # respond to the LLM with user feedback elif response[\"type\"] == \"response\": user_feedback = response[\"args\"] tool_response = user_feedback else: raise ValueError(f\"Unsupported interrupt response type: {response['type']}\") return tool_response return call_tool_with_interrupt ``` This wrapper creates a new tool that calls interrupt() before executing the wrapped tool. interrupt() is using special input and output format that's expected by Agent Inbox UI: a list of [HumanInterrupt][langgraph.prebuilt.interrupt.HumanInterrupt] objects is sent to AgentInbox render interrupt information to the end user resume value is provided by AgentInbox as a list (i.e., Command(resume=[...])) You can use the add_human_in_the_loop wrapper to add interrupt() to any tool without having to add it inside the tool: ```python from langgraph.checkpoint.memory import InMemorySaver from langgraph.prebuilt import create_react_agent highlight-next-line checkpointer = InMemorySaver() def book_hotel(hotel_name: str): \"\"\"Book a hotel\"\"\" return f\"Successfully booked a stay at {hotel_name}.\" agent = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", tools=[ # highlight-next-line add_human_in_the_loop(book_hotel), # (1)! ], # highlight-next-line checkpointer=checkpointer, ) config = {\"configurable\": {\"thread_id\": \"1\"}} Run the agent for chunk in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]}, # highlight-next-line config ): print(chunk) print(\"\\n\") ``` The add_human_in_the_loop wrapper is used to add interrupt() to the tool. This allows the agent to pause execution and wait for human input before proceeding with the tool call. You should see that the agent runs until it reaches the interrupt() call, at which point it pauses and waits for human input. Resume the agent with a Command(resume=...) to continue based on human input. ```python from langgraph.types import Command for chunk in agent.stream( # highlight-next-line Command(resume=[{\"type\": \"accept\"}]), # Command(resume=[{\"type\": \"edit\", \"args\": {\"args\": {\"hotel_name\": \"McKittrick Hotel\"}}}]), config ): print(chunk) print(\"\\n\") ```"],"reference":"The LangGraph agent utilizes the human-in-the-loop feature by incorporating the interrupt() function within the tool that handles hotel bookings. When a user requests to book a stay at the McKittrick hotel, the agent pauses execution at the interrupt() call, allowing for human review and approval of the tool call. The process involves the agent waiting for human input, which can either be an acceptance of the booking or an edit request. Once the human input is received, the agent can resume its operation using the Command(resume=...) function, which allows it to continue based on the provided human feedback. This ensures that the booking process is validated and can be adjusted as necessary before finalizing the hotel reservation.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"What are the steps to set up an agent using LangGraph and how can model fallbacks be implemented with LangChain?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\nAdding model fallbacks You can add a fallback to a different model or a different LLM provider using model.with_fallbacks([...]): === \"init_chat_model\" ```python from langchain.chat_models import init_chat_model model_with_fallbacks = ( init_chat_model(\"anthropic:claude-3-5-haiku-latest\") # highlight-next-line .with_fallbacks([ init_chat_model(\"openai:gpt-4.1-mini\"), ]) ) ``` === \"ChatModel\" ```python from langchain_anthropic import ChatAnthropic from langchain_openai import ChatOpenAI model_with_fallbacks = ( ChatAnthropic(model=\"claude-3-5-haiku-latest\") # highlight-next-line .with_fallbacks([ ChatOpenAI(model=\"gpt-4.1-mini\"), ]) ) ``` See this guide for more information on model fallbacks. Additional resources Model integration directory Universal initialization with init_chat_model"],"reference":"To set up an agent using LangGraph, you need to follow these steps: First, ensure you have an Anthropic API key and install the necessary dependencies by running 'pip install -U langgraph \"langchain[anthropic]\"'. Next, create an agent using the 'create_react_agent' function from LangGraph's prebuilt components. You can define a tool for the agent, such as a function to get the weather for a given city, and provide a language model for the agent to use. After defining the agent, you can run it with the 'invoke' method. For implementing model fallbacks with LangChain, you can use the 'model.with_fallbacks([...])' method to add a fallback to a different model or LLM provider. This allows the agent to switch to an alternative model if the primary one fails, ensuring more reliable performance.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How can you configure an LLM with specific parameters and handle tool errors using the model 'anthropic:claude-3-7-sonnet-latest'?","reference_contexts":["<1-hop>\n\nHandle tool errors By default, the agent will catch all exceptions raised during tool calls and will pass those as tool messages to the LLM. To control how the errors are handled, you can use the prebuilt [ToolNode][langgraph.prebuilt.tool_node.ToolNode] \u2014 the node that executes tools inside create_react_agent \u2014 via its handle_tool_errors parameter: === \"Enable error handling (default)\" ```python from langgraph.prebuilt import create_react_agent def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" if a == 42: raise ValueError(\"The ultimate error\") return a * b # Run with error handling (default) agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[multiply] ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]} ) ``` === \"Disable error handling\" ```python from langgraph.prebuilt import create_react_agent, ToolNode def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" if a == 42: raise ValueError(\"The ultimate error\") return a * b # highlight-next-line tool_node = ToolNode( [multiply], # highlight-next-line handle_tool_errors=False # (1)! ) agent_no_error_handling = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=tool_node ) agent_no_error_handling.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]} ) ``` 1. This disables error handling (enabled by default). See all available strategies in the [API reference][langgraph.prebuilt.tool_node.ToolNode]. === \"Custom error handling\" ```python from langgraph.prebuilt import create_react_agent, ToolNode def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" if a == 42: raise ValueError(\"The ultimate error\") return a * b # highlight-next-line tool_node = ToolNode( [multiply], # highlight-next-line handle_tool_errors=( \"Can't use 42 as a first operand, you must switch operands!\" # (1)! ) ) agent_custom_error_handling = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=tool_node ) agent_custom_error_handling.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]} ) ``` 1. This provides a custom message to send to the LLM in case of an exception. See all available strategies in the [API reference][langgraph.prebuilt.tool_node.ToolNode]. See [API reference][langgraph.prebuilt.tool_node.ToolNode] for more information on different tool error handling options. Working with memory LangGraph allows access to short-term and long-term memory from tools. See Memory guide for more information on: how to read from and write to short-term memory how to read from and write to long-term memory Prebuilt tools You can use prebuilt tools from model providers by passing a dictionary with tool specs to the tools parameter of create_react_agent. For example, to use the web_search_preview tool from OpenAI: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"openai:gpt-4o-mini\", tools=[{\"type\": \"web_search_preview\"}] ) response = agent.invoke( {\"messages\": [\"What was a positive news story from today?\"]} ) ``` Additionally, LangChain supports a wide range of prebuilt tool integrations for interacting with APIs, databases, file systems, web data, and more. These tools extend the functionality of agents and enable rapid development. You can browse the full list of available integrations in the LangChain integrations directory. Some commonly used tool categories include: Search: Bing, SerpAPI, Tavily Code interpreters: Python REPL, Node.js REPL Databases: SQL, MongoDB, Redis Web data: Web scraping and browsing APIs: OpenWeatherMap, NewsAPI, and others These integrations can be configured and added to your agents using the same tools parameter shown in the examples above.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"To configure an LLM with specific parameters such as temperature, you can use the init_chat_model function. For example, you can initialize the model with a temperature setting like this: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent model = init_chat_model( 'anthropic:claude-3-7-sonnet-latest', temperature=0 ) agent = create_react_agent( model=model, tools=[get_weather], ) ``` Additionally, to handle tool errors, you can use the ToolNode with the handle_tool_errors parameter. By default, the agent will catch all exceptions raised during tool calls and pass those as tool messages to the LLM. You can enable or disable error handling as needed, for instance: ```python tool_node = ToolNode( [multiply], handle_tool_errors=False ) ``` This allows you to customize how errors are managed during the execution of tools.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How can LangGraph's components, specifically the Claude-3-5-sonnet-latest and Claude-3-7-sonnet-latest models, be utilized in a multi-agent system for effective task management?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags Multi-agent A single agent might struggle if it needs to specialize in multiple domains or manage many tools. To tackle this, you can break your agent into smaller, independent agents and composing them into a multi-agent system. In multi-agent systems, agents need to communicate between each other. They do so via handoffs \u2014 a primitive that describes which agent to hand control to and the payload to send to that agent. Two of the most popular multi-agent architectures are: supervisor \u2014 individual agents are coordinated by a central supervisor agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements. swarm \u2014 agents dynamically hand off control to one another based on their specializations. The system remembers which agent was last active, ensuring that on subsequent interactions, the conversation resumes with that agent. Supervisor Supervisor Use langgraph-supervisor library to create a supervisor multi-agent system: bash pip install langgraph-supervisor ```python from langchain_openai import ChatOpenAI from langgraph.prebuilt import create_react_agent highlight-next-line from langgraph_supervisor import create_supervisor def book_hotel(hotel_name: str): \"\"\"Book a hotel\"\"\" return f\"Successfully booked a stay at {hotel_name}.\" def book_flight(from_airport: str, to_airport: str): \"\"\"Book a flight\"\"\" return f\"Successfully booked a flight from {from_airport} to {to_airport}.\" flight_assistant = create_react_agent( model=\"openai:gpt-4o\", tools=[book_flight], prompt=\"You are a flight booking assistant\", # highlight-next-line name=\"flight_assistant\" ) hotel_assistant = create_react_agent( model=\"openai:gpt-4o\", tools=[book_hotel], prompt=\"You are a hotel booking assistant\", # highlight-next-line name=\"hotel_assistant\" ) highlight-next-line supervisor = create_supervisor( agents=[flight_assistant, hotel_assistant], model=ChatOpenAI(model=\"gpt-4o\"), prompt=( \"You manage a hotel booking assistant and a\" \"flight booking assistant. Assign work to them.\" ) ).compile() for chunk in supervisor.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\" } ] } ): print(chunk) print(\"\\n\") ``` Swarm Swarm Use langgraph-swarm library to create a swarm multi-agent system: bash pip install langgraph-swarm ```python from langgraph.prebuilt import create_react_agent highlight-next-line from langgraph_swarm import create_swarm, create_handoff_tool transfer_to_hotel_assistant = create_handoff_tool( agent_name=\"hotel_assistant\", description=\"Transfer user to the hotel-booking assistant.\", ) transfer_to_flight_assistant = create_handoff_tool( agent_name=\"flight_assistant\", description=\"Transfer user to the flight-booking assistant.\", ) flight_assistant = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", # highlight-next-line tools=[book_flight, transfer_to_hotel_assistant], prompt=\"You are a flight booking assistant\", # highlight-next-line name=\"flight_assistant\" ) hotel_assistant = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", # highlight-next-line tools=[book_hotel, transfer_to_flight_assistant], prompt=\"You are a hotel booking assistant\", # highlight-next-line name=\"hotel_assistant\" ) highlight-next-line swarm = create_swarm( agents=[flight_assistant, hotel_assistant], default_active_agent=\"flight_assistant\" ).compile() for chunk in swarm.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\" } ] } ): print(chunk) print(\"\\n\") ``` Handoffs A common pattern in multi-agent interactions is handoffs, where one agent hands off control to another. Handoffs allow you to specify: destination: target agent to navigate to payload: information to pass to that agent This is used both by langgraph-supervisor (supervisor hands off to individual agents) and langgraph-swarm (an individual agent can hand off to other agents). To implement handoffs with create_react_agent, you need to: Create a special tool that can transfer control to a different agent python def transfer_to_bob(): \"\"\"Transfer to bob.\"\"\" return Command( # name of the agent (node) to go to # highlight-next-line goto=\"bob\", # data to send to the agent # highlight-next-line update={\"messages\": [...]}, # indicate to LangGraph that we need to navigate to # agent node in a parent graph # highlight-next-line graph=Command.PARENT, ) Create individual agents that have access to handoff tools: python flight_assistant = create_react_agent( ..., tools=[book_flight, transfer_to_hotel_assistant] ) hotel_assistant = create_react_agent( ..., tools=[book_hotel, transfer_to_flight_assistant] ) Define a parent graph that contains individual agents as nodes: python from langgraph.graph import StateGraph, MessagesState multi_agent_graph = ( StateGraph(MessagesState) .add_node(flight_assistant) .add_node(hotel_assistant) ... ) Putting this together, here is how you can implement a simple multi-agent system with two agents \u2014 a flight booking assistant and a hotel booking assistant: ```python from typing import Annotated from langchain_core.tools import tool, InjectedToolCallId from langgraph.prebuilt import create_react_agent, InjectedState from langgraph.graph import StateGraph, START, MessagesState from langgraph.types import Command def create_handoff_tool(*, agent_name: str, description: str | None = None): name = f\"transfer_to_{agent_name}\" description = description or f\"Transfer to {agent_name}\" @tool(name, description=description) def handoff_tool( # highlight-next-line state: Annotated[MessagesState, InjectedState], # (1)! # highlight-next-line tool_call_id: Annotated[str, InjectedToolCallId], ) -> Command: tool_message = { \"role\": \"tool\", \"content\": f\"Successfully transferred to {agent_name}\", \"name\": name, \"tool_call_id\": tool_call_id, } return Command( # (2)! # highlight-next-line goto=agent_name, # (3)! # highlight-next-line update={\"messages\": state[\"messages\"] + [tool_message]}, # (4)! # highlight-next-line graph=Command.PARENT, # (5)! ) return handoff_tool Handoffs transfer_to_hotel_assistant = create_handoff_tool( agent_name=\"hotel_assistant\", description=\"Transfer user to the hotel-booking assistant.\", ) transfer_to_flight_assistant = create_handoff_tool( agent_name=\"flight_assistant\", description=\"Transfer user to the flight-booking assistant.\", )"],"reference":"LangGraph's components can be utilized in a multi-agent system by creating independent agents that specialize in different tasks, such as flight and hotel booking. For instance, the Claude-3-5-sonnet-latest model can be used for a flight booking assistant, while the Claude-3-7-sonnet-latest model can serve as a hotel booking assistant. These agents can communicate through handoffs, allowing them to transfer control and information seamlessly. This setup enhances task management by enabling agents to focus on their specific domains while collaborating effectively.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How does LangGraph utilize LangChain for managing long-term memory and enabling semantic search in agent configurations?","reference_contexts":["<1-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart","<2-hop>\n\nSemantic search LangGraph also allows you to search for items in long-term memory by semantic similarity. Prebuilt memory tools LangMem is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the LangMem documentation for usage examples. Additional resources Memory in LangGraph"],"reference":"LangGraph utilizes LangChain through its library LangMem, which offers tools for managing long-term memories in agents. This allows agents to perform semantic searches for items in their long-term memory by semantic similarity. The integration of LangChain facilitates the configuration of agents to include memory capabilities, enabling them to store and retrieve information effectively during multi-turn conversations.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How does LangChain utilize the langchain_core library to enhance tool functionality and manage agent interactions?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags Tools Tools are a way to encapsulate a function and its input schema in a way that can be passed to a chat model that supports tool calling. This allows the model to request the execution of this function with specific inputs. You can either define your own tools or use prebuilt integrations that LangChain provides. Define simple tools You can pass a vanilla function to create_react_agent to use as a tool: ```python from langgraph.prebuilt import create_react_agent def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * b create_react_agent( model=\"anthropic:claude-3-7-sonnet\", tools=[multiply] ) ``` create_react_agent automatically converts vanilla functions to LangChain tools. Customize tools For more control over tool behavior, use the @tool decorator: ```python highlight-next-line from langchain_core.tools import tool highlight-next-line @tool(\"multiply_tool\", parse_docstring=True) def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers. Args: a: First operand b: Second operand \"\"\" return a * b ``` You can also define a custom input schema using Pydantic: ```python from pydantic import BaseModel, Field class MultiplyInputSchema(BaseModel): \"\"\"Multiply two numbers\"\"\" a: int = Field(description=\"First operand\") b: int = Field(description=\"Second operand\") highlight-next-line @tool(\"multiply_tool\", args_schema=MultiplyInputSchema) def multiply(a: int, b: int) -> int: return a * b ``` For additional customization, refer to the custom tools guide. Hide arguments from the model Some tools require runtime-only arguments (e.g., user ID or session context) that should not be controllable by the model. You can put these arguments in the state or config of the agent, and access this information inside the tool: ```python from langgraph.prebuilt import InjectedState from langgraph.prebuilt.chat_agent_executor import AgentState from langchain_core.runnables import RunnableConfig def my_tool( # This will be populated by an LLM tool_arg: str, # access information that's dynamically updated inside the agent # highlight-next-line state: Annotated[AgentState, InjectedState], # access static data that is passed at agent invocation # highlight-next-line config: RunnableConfig, ) -> str: \"\"\"My tool.\"\"\" do_something_with_state(state[\"messages\"]) do_something_with_config(config) ... ``` Disable parallel tool calling Some model providers support executing multiple tools in parallel, but allow users to disable this feature. For supported providers, you can disable parallel tool calling by setting parallel_tool_calls=False via the model.bind_tools() method: ```python from langchain.chat_models import init_chat_model def add(a: int, b: int) -> int: \"\"\"Add two numbers\"\"\" return a + b def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * b model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0) tools = [add, multiply] agent = create_react_agent( # disable parallel tool calls # highlight-next-line model=model.bind_tools(tools, parallel_tool_calls=False), tools=tools ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 3 + 5 and 4 * 7?\"}]} ) ``` Return tool results directly Use return_direct=True to return tool results immediately and stop the agent loop: ```python from langchain_core.tools import tool highlight-next-line @tool(return_direct=True) def add(a: int, b: int) -> int: \"\"\"Add two numbers\"\"\" return a + b agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[add] ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 3 + 5?\"}]} ) ``` Force tool use To force the agent to use specific tools, you can set the tool_choice option in model.bind_tools(): ```python from langchain_core.tools import tool highlight-next-line @tool(return_direct=True) def greet(user_name: str) -> int: \"\"\"Greet user.\"\"\" return f\"Hello {user_name}!\" tools = [greet] agent = create_react_agent( # highlight-next-line model=model.bind_tools(tools, tool_choice={\"type\": \"tool\", \"name\": \"greet\"}), tools=tools ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Hi, I am Bob\"}]} ) ``` !!! Warning \"Avoid infinite loops\" Forcing tool usage without stopping conditions can create infinite loops. Use one of the following safeguards: - Mark the tool with [`return_direct=True`](#return-tool-results-directly) to end the loop after execution. - Set [`recursion_limit`](..\/concepts\/low_level.md#recursion-limit) to restrict the number of execution steps.","<2-hop>\n\nRun the agent with the stream() method, passing the config object to specify the thread ID. This allows the agent to resume the same conversation on future invocations. ```python config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" } } for chunk in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]}, # highlight-next-line config ): print(chunk) print(\"\\n\") ``` You should see that the agent runs until it reaches the interrupt() call, at which point it pauses and waits for human input. Resume the agent with a Command(resume=...) to continue based on human input. ```python from langgraph.types import Command for chunk in agent.stream( # highlight-next-line Command(resume={\"type\": \"accept\"}), # (1)! # Command(resume={\"type\": \"edit\", \"args\": {\"hotel_name\": \"McKittrick Hotel\"}}), config ): print(chunk) print(\"\\n\") ``` The [interrupt function][langgraph.types.interrupt] is used in conjunction with the [Command][langgraph.types.Command] object to resume the graph with a value provided by the human. Using with Agent Inbox You can create a wrapper to add interrupts to any tool. The example below provides a reference implementation compatible with Agent Inbox UI and Agent Chat UI. ```python title=\"Wrapper that adds human-in-the-loop to any tool\" from typing import Callable from langchain_core.tools import BaseTool, tool as create_tool from langchain_core.runnables import RunnableConfig from langgraph.types import interrupt from langgraph.prebuilt.interrupt import HumanInterruptConfig, HumanInterrupt def add_human_in_the_loop( tool: Callable | BaseTool, *, interrupt_config: HumanInterruptConfig = None, ) -> BaseTool: \"\"\"Wrap a tool to support human-in-the-loop review.\"\"\" if not isinstance(tool, BaseTool): tool = create_tool(tool) if interrupt_config is None: interrupt_config = { \"allow_accept\": True, \"allow_edit\": True, \"allow_respond\": True, } @create_tool( # (1)! tool.name, description=tool.description, args_schema=tool.args_schema ) def call_tool_with_interrupt(config: RunnableConfig, **tool_input): request: HumanInterrupt = { \"action_request\": { \"action\": tool.name, \"args\": tool_input }, \"config\": interrupt_config, \"description\": \"Please review the tool call\" } # highlight-next-line response = interrupt([request])[0] # (2)! # approve the tool call if response[\"type\"] == \"accept\": tool_response = tool.invoke(tool_input, config) # update tool call args elif response[\"type\"] == \"edit\": tool_input = response[\"args\"][\"args\"] tool_response = tool.invoke(tool_input, config) # respond to the LLM with user feedback elif response[\"type\"] == \"response\": user_feedback = response[\"args\"] tool_response = user_feedback else: raise ValueError(f\"Unsupported interrupt response type: {response['type']}\") return tool_response return call_tool_with_interrupt ``` This wrapper creates a new tool that calls interrupt() before executing the wrapped tool. interrupt() is using special input and output format that's expected by Agent Inbox UI: a list of [HumanInterrupt][langgraph.prebuilt.interrupt.HumanInterrupt] objects is sent to AgentInbox render interrupt information to the end user resume value is provided by AgentInbox as a list (i.e., Command(resume=[...])) You can use the add_human_in_the_loop wrapper to add interrupt() to any tool without having to add it inside the tool: ```python from langgraph.checkpoint.memory import InMemorySaver from langgraph.prebuilt import create_react_agent highlight-next-line checkpointer = InMemorySaver() def book_hotel(hotel_name: str): \"\"\"Book a hotel\"\"\" return f\"Successfully booked a stay at {hotel_name}.\" agent = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", tools=[ # highlight-next-line add_human_in_the_loop(book_hotel), # (1)! ], # highlight-next-line checkpointer=checkpointer, ) config = {\"configurable\": {\"thread_id\": \"1\"}} Run the agent for chunk in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]}, # highlight-next-line config ): print(chunk) print(\"\\n\") ``` The add_human_in_the_loop wrapper is used to add interrupt() to the tool. This allows the agent to pause execution and wait for human input before proceeding with the tool call. You should see that the agent runs until it reaches the interrupt() call, at which point it pauses and waits for human input. Resume the agent with a Command(resume=...) to continue based on human input. ```python from langgraph.types import Command for chunk in agent.stream( # highlight-next-line Command(resume=[{\"type\": \"accept\"}]), # Command(resume=[{\"type\": \"edit\", \"args\": {\"args\": {\"hotel_name\": \"McKittrick Hotel\"}}}]), config ): print(chunk) print(\"\\n\") ```"],"reference":"LangChain utilizes the langchain_core library to enhance tool functionality by allowing the definition of custom tools with specific input schemas and behaviors. For instance, tools can be created using the @tool decorator, which enables customization of tool behavior and input validation. Additionally, LangChain supports managing agent interactions through features like the stream() method, which allows agents to resume conversations and handle human input dynamically. This integration of langchain_core facilitates a more robust and interactive experience when using tools within LangChain.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How can you use LangChain with LangGraph to create an agent that incorporates human-in-the-loop functionality?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\nRun the agent with the stream() method, passing the config object to specify the thread ID. This allows the agent to resume the same conversation on future invocations. ```python config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" } } for chunk in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]}, # highlight-next-line config ): print(chunk) print(\"\\n\") ``` You should see that the agent runs until it reaches the interrupt() call, at which point it pauses and waits for human input. Resume the agent with a Command(resume=...) to continue based on human input. ```python from langgraph.types import Command for chunk in agent.stream( # highlight-next-line Command(resume={\"type\": \"accept\"}), # (1)! # Command(resume={\"type\": \"edit\", \"args\": {\"hotel_name\": \"McKittrick Hotel\"}}), config ): print(chunk) print(\"\\n\") ``` The [interrupt function][langgraph.types.interrupt] is used in conjunction with the [Command][langgraph.types.Command] object to resume the graph with a value provided by the human. Using with Agent Inbox You can create a wrapper to add interrupts to any tool. The example below provides a reference implementation compatible with Agent Inbox UI and Agent Chat UI. ```python title=\"Wrapper that adds human-in-the-loop to any tool\" from typing import Callable from langchain_core.tools import BaseTool, tool as create_tool from langchain_core.runnables import RunnableConfig from langgraph.types import interrupt from langgraph.prebuilt.interrupt import HumanInterruptConfig, HumanInterrupt def add_human_in_the_loop( tool: Callable | BaseTool, *, interrupt_config: HumanInterruptConfig = None, ) -> BaseTool: \"\"\"Wrap a tool to support human-in-the-loop review.\"\"\" if not isinstance(tool, BaseTool): tool = create_tool(tool) if interrupt_config is None: interrupt_config = { \"allow_accept\": True, \"allow_edit\": True, \"allow_respond\": True, } @create_tool( # (1)! tool.name, description=tool.description, args_schema=tool.args_schema ) def call_tool_with_interrupt(config: RunnableConfig, **tool_input): request: HumanInterrupt = { \"action_request\": { \"action\": tool.name, \"args\": tool_input }, \"config\": interrupt_config, \"description\": \"Please review the tool call\" } # highlight-next-line response = interrupt([request])[0] # (2)! # approve the tool call if response[\"type\"] == \"accept\": tool_response = tool.invoke(tool_input, config) # update tool call args elif response[\"type\"] == \"edit\": tool_input = response[\"args\"][\"args\"] tool_response = tool.invoke(tool_input, config) # respond to the LLM with user feedback elif response[\"type\"] == \"response\": user_feedback = response[\"args\"] tool_response = user_feedback else: raise ValueError(f\"Unsupported interrupt response type: {response['type']}\") return tool_response return call_tool_with_interrupt ``` This wrapper creates a new tool that calls interrupt() before executing the wrapped tool. interrupt() is using special input and output format that's expected by Agent Inbox UI: a list of [HumanInterrupt][langgraph.prebuilt.interrupt.HumanInterrupt] objects is sent to AgentInbox render interrupt information to the end user resume value is provided by AgentInbox as a list (i.e., Command(resume=[...])) You can use the add_human_in_the_loop wrapper to add interrupt() to any tool without having to add it inside the tool: ```python from langgraph.checkpoint.memory import InMemorySaver from langgraph.prebuilt import create_react_agent highlight-next-line checkpointer = InMemorySaver() def book_hotel(hotel_name: str): \"\"\"Book a hotel\"\"\" return f\"Successfully booked a stay at {hotel_name}.\" agent = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", tools=[ # highlight-next-line add_human_in_the_loop(book_hotel), # (1)! ], # highlight-next-line checkpointer=checkpointer, ) config = {\"configurable\": {\"thread_id\": \"1\"}} Run the agent for chunk in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]}, # highlight-next-line config ): print(chunk) print(\"\\n\") ``` The add_human_in_the_loop wrapper is used to add interrupt() to the tool. This allows the agent to pause execution and wait for human input before proceeding with the tool call. You should see that the agent runs until it reaches the interrupt() call, at which point it pauses and waits for human input. Resume the agent with a Command(resume=...) to continue based on human input. ```python from langgraph.types import Command for chunk in agent.stream( # highlight-next-line Command(resume=[{\"type\": \"accept\"}]), # Command(resume=[{\"type\": \"edit\", \"args\": {\"args\": {\"hotel_name\": \"McKittrick Hotel\"}}}]), config ): print(chunk) print(\"\\n\") ```"],"reference":"To use LangChain with LangGraph to create an agent that incorporates human-in-the-loop functionality, you first need to set up the agent using LangGraph's prebuilt components. You can create an agent with the `create_react_agent` function, specifying the model and tools. For human-in-the-loop, you can wrap a tool using the `add_human_in_the_loop` function, which allows the agent to pause execution and wait for human input before proceeding. This is done by using the `interrupt()` function in conjunction with the `Command` object to resume the agent based on the user's feedback.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How can I use the InMemorySaver to enable long-term memory in LangGraph while configuring an LLM with specific parameters?","reference_contexts":["<1-hop>\n\nLong-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. Manage message history Long conversations can exceed the LLM's context window. Common solutions are: Summarization: Maintain a running summary of the conversation Trimming: Remove first or last N messages in the history This allows the agent to keep track of the conversation without exceeding the LLM's context window. To manage message history, specify pre_model_hook \u2014 a function (node) that will always run before calling the language model. Summarize message history To summarize message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with a prebuilt SummarizationNode: ```python from langchain_anthropic import ChatAnthropic from langmem.short_term import SummarizationNode from langchain_core.messages.utils import count_tokens_approximately from langgraph.prebuilt import create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.checkpoint.memory import InMemorySaver from typing import Any model = ChatAnthropic(model=\"claude-3-7-sonnet-latest\") summarization_node = SummarizationNode( # (1)! token_counter=count_tokens_approximately, model=model, max_tokens=384, max_summary_tokens=128, output_messages_key=\"llm_input_messages\", ) class State(AgentState): # NOTE: we're adding this key to keep track of previous summary information # to make sure we're not summarizing on every LLM call # highlight-next-line context: dict[str, Any] # (2)! checkpointer = InMemorySaver() # (3)! agent = create_react_agent( model=model, tools=tools, # highlight-next-line pre_model_hook=summarization_node, # (4)! # highlight-next-line state_schema=State, # (5)! checkpointer=checkpointer, ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The context key is added to the agent's state. The key contains book-keeping information for the summarization node. It is used to keep track of the last summary information and ensure that the agent doesn't summarize on every LLM call, which can be inefficient. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. The pre_model_hook is set to the SummarizationNode. This node will summarize the message history before sending it to the LLM. The summarization node will automatically handle the summarization process and update the agent's state with the new summary. You can replace this with a custom implementation if you prefer. Please see the [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent] API reference for more details. The state_schema is set to the State class, which is the custom state that contains an extra context key. Trim message history To trim message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with trim_messages function: ```python highlight-next-line from langchain_core.messages.utils import ( # highlight-next-line trim_messages, # highlight-next-line count_tokens_approximately highlight-next-line ) from langgraph.prebuilt import create_react_agent This function will be called every time before the node that calls LLM def pre_model_hook(state): trimmed_messages = trim_messages( state[\"messages\"], strategy=\"last\", token_counter=count_tokens_approximately, max_tokens=384, start_on=\"human\", end_on=(\"human\", \"tool\"), ) # highlight-next-line return {\"llm_input_messages\": trimmed_messages} checkpointer = InMemorySaver() agent = create_react_agent( model, tools, # highlight-next-line pre_model_hook=pre_model_hook, checkpointer=checkpointer, ) ``` To learn more about using pre_model_hook for managing message history, see this how-to guide Read in tools { #read-short-term } LangGraph allows agent to access its short-term memory (state) inside the tools. ```python from typing import Annotated from langgraph.prebuilt import InjectedState, create_react_agent class CustomState(AgentState): # highlight-next-line user_id: str def get_user_info( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Look up user info.\"\"\" # highlight-next-line user_id = state[\"user_id\"] return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line state_schema=CustomState, ) agent.invoke({ \"messages\": \"look up user information\", # highlight-next-line \"user_id\": \"user_123\" }) ``` See the Context guide for more information. Write from tools { #write-short-term } To modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"To use the InMemorySaver for enabling long-term memory in LangGraph while configuring an LLM with specific parameters, you first need to create an instance of InMemorySaver as your checkpointer. This allows the agent to store its state in memory. When configuring the LLM, you can use the init_chat_model function to set specific parameters like temperature. For example, you can initialize the model with the following code: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent model = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\", temperature=0) checkpointer = InMemorySaver() agent = create_react_agent(model=model, tools=[get_weather], checkpointer=checkpointer) ``` This setup enables the agent to persist its state across invocations, allowing for multi-turn conversations and the use of long-term memory.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How does LangGraph utilize short-term memory to manage conversations about weather in sf and new york, and what role does the checkpointer play in this process?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags Memory LangGraph supports two types of memory essential for building conversational agents: Short-term memory: Tracks the ongoing conversation by maintaining message history within a session. Long-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. ","<2-hop>\n\nLong-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. Manage message history Long conversations can exceed the LLM's context window. Common solutions are: Summarization: Maintain a running summary of the conversation Trimming: Remove first or last N messages in the history This allows the agent to keep track of the conversation without exceeding the LLM's context window. To manage message history, specify pre_model_hook \u2014 a function (node) that will always run before calling the language model. Summarize message history To summarize message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with a prebuilt SummarizationNode: ```python from langchain_anthropic import ChatAnthropic from langmem.short_term import SummarizationNode from langchain_core.messages.utils import count_tokens_approximately from langgraph.prebuilt import create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.checkpoint.memory import InMemorySaver from typing import Any model = ChatAnthropic(model=\"claude-3-7-sonnet-latest\") summarization_node = SummarizationNode( # (1)! token_counter=count_tokens_approximately, model=model, max_tokens=384, max_summary_tokens=128, output_messages_key=\"llm_input_messages\", ) class State(AgentState): # NOTE: we're adding this key to keep track of previous summary information # to make sure we're not summarizing on every LLM call # highlight-next-line context: dict[str, Any] # (2)! checkpointer = InMemorySaver() # (3)! agent = create_react_agent( model=model, tools=tools, # highlight-next-line pre_model_hook=summarization_node, # (4)! # highlight-next-line state_schema=State, # (5)! checkpointer=checkpointer, ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The context key is added to the agent's state. The key contains book-keeping information for the summarization node. It is used to keep track of the last summary information and ensure that the agent doesn't summarize on every LLM call, which can be inefficient. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. The pre_model_hook is set to the SummarizationNode. This node will summarize the message history before sending it to the LLM. The summarization node will automatically handle the summarization process and update the agent's state with the new summary. You can replace this with a custom implementation if you prefer. Please see the [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent] API reference for more details. The state_schema is set to the State class, which is the custom state that contains an extra context key. Trim message history To trim message history, you can use [pre_model_hook][langgraph.prebuilt.chat_agent_executor.create_react_agent] with trim_messages function: ```python highlight-next-line from langchain_core.messages.utils import ( # highlight-next-line trim_messages, # highlight-next-line count_tokens_approximately highlight-next-line ) from langgraph.prebuilt import create_react_agent This function will be called every time before the node that calls LLM def pre_model_hook(state): trimmed_messages = trim_messages( state[\"messages\"], strategy=\"last\", token_counter=count_tokens_approximately, max_tokens=384, start_on=\"human\", end_on=(\"human\", \"tool\"), ) # highlight-next-line return {\"llm_input_messages\": trimmed_messages} checkpointer = InMemorySaver() agent = create_react_agent( model, tools, # highlight-next-line pre_model_hook=pre_model_hook, checkpointer=checkpointer, ) ``` To learn more about using pre_model_hook for managing message history, see this how-to guide Read in tools { #read-short-term } LangGraph allows agent to access its short-term memory (state) inside the tools. ```python from typing import Annotated from langgraph.prebuilt import InjectedState, create_react_agent class CustomState(AgentState): # highlight-next-line user_id: str def get_user_info( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Look up user info.\"\"\" # highlight-next-line user_id = state[\"user_id\"] return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line state_schema=CustomState, ) agent.invoke({ \"messages\": \"look up user information\", # highlight-next-line \"user_id\": \"user_123\" }) ``` See the Context guide for more information. Write from tools { #write-short-term } To modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to"],"reference":"LangGraph utilizes short-term memory, also referred to as thread-level memory, to manage conversations by tracking ongoing discussions through message history within a session. When a user asks about the weather in sf, the agent maintains the context of that conversation using a unique thread_id. The checkpointer, specifically the InMemorySaver, is crucial as it enables the persistence of the agent's state across invocations. This means that when the user continues the conversation by asking about the weather in new york, the agent can infer the context from the previous messages stored in short-term memory, allowing for a seamless interaction.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How does the InMemorySaver facilitate both short-term memory and human-in-the-loop capabilities in LangGraph agents, and what role does it play in maintaining conversation context across multiple invocations?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags Memory LangGraph supports two types of memory essential for building conversational agents: Short-term memory: Tracks the ongoing conversation by maintaining message history within a session. Long-term memory: Stores user-specific or application-level data across sessions. This guide demonstrates how to use both memory types with agents in LangGraph. For a deeper understanding of memory concepts, refer to the LangGraph memory documentation. !!! note \"Terminology\" In LangGraph: - *Short-term memory* is also referred to as **thread-level memory**. - *Long-term memory* is also called **cross-thread memory**. A [thread](..\/concepts\/persistence.md#threads) represents a sequence of related runs grouped by the same `thread_id`. Short-term memory Short-term memory enables agents to track multi-turn conversations. To use it, you must: Provide a checkpointer when creating the agent. The checkpointer enables persistence of the agent's state. Supply a thread_id in the config when running the agent. The thread_id is a unique identifier for the conversation session. ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() # (1)! def get_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (2)! ) Run the agent config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" # (3)! } } sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config ) Continue the conversation using the same thread_id ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config # (4)! ) ``` The InMemorySaver is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you. The checkpointer is passed to the agent. This enables the agent to persist its state across invocations. A unique thread_id is provided in the config. This ID is used to identify the conversation session. The value is controlled by the user and can be any string. The agent will continue the conversation using the same thread_id. This will allow the agent to infer that the user is asking specifically about the weather in New York. When the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, allowing the agent to infer that the user is asking specifically about the weather in New York. !!! Note \"LangGraph Platform providers a production-ready checkpointer\" If you're using [LangGraph Platform](.\/deployment.md), during deployment your checkpointer will be automatically configured to use a production-ready database. ","<2-hop>\n\nsearch: boost: 2 tags: - human-in-the-loop - hil - agent hide: - tags Human-in-the-loop To review, edit and approve tool calls in an agent you can use LangGraph's built-in Human-In-the-Loop (HIL) features, specifically the [interrupt()][langgraph.types.interrupt] primitive. LangGraph allows you to pause execution indefinitely \u2014 for minutes, hours, or even days\u2014until human input is received. This is possible because the agent state is checkpointed into a database, which allows the system to persist execution context and later resume the workflow, continuing from where it left off. For a deeper dive into the human-in-the-loop concept, see the concept guide. Review tool calls To add a human approval step to a tool: Use interrupt() in the tool to pause execution. Resume with a Command(resume=...) to continue based on human input. ```python from langgraph.checkpoint.memory import InMemorySaver from langgraph.types import interrupt from langgraph.prebuilt import create_react_agent An example of a sensitive tool that requires human review \/ approval def book_hotel(hotel_name: str): \"\"\"Book a hotel\"\"\" # highlight-next-line response = interrupt( # (1)! f\"Trying to call book_hotel with args {{'hotel_name': {hotel_name}}}. \" \"Please approve or suggest edits.\" ) if response[\"type\"] == \"accept\": pass elif response[\"type\"] == \"edit\": hotel_name = response[\"args\"][\"hotel_name\"] else: raise ValueError(f\"Unknown response type: {response['type']}\") return f\"Successfully booked a stay at {hotel_name}.\" highlight-next-line checkpointer = InMemorySaver() # (2)! agent = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", tools=[book_hotel], # highlight-next-line checkpointer=checkpointer, # (3)! ) ``` The [interrupt function][langgraph.types.interrupt] pauses the agent graph at a specific node. In this case, we call interrupt() at the beginning of the tool function, which pauses the graph at the node that executes the tool. The information inside interrupt() (e.g., tool calls) can be presented to a human, and the graph can be resumed with the user input (tool call approval, edit or feedback). The InMemorySaver is used to store the agent state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. In this example, we use InMemorySaver to store the agent state in memory. In a production application, the agent state will be stored in a database. Initialize the agent with the checkpointer. Run the agent with the stream() method, passing the config object to specify the thread ID. This allows the agent to resume the same conversation on future invocations. ```python config = { \"configurable\": { # highlight-next-line \"thread_id\": \"1\" } } for chunk in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]}, # highlight-next-line config ): print(chunk) print(\"\\n\") ``` You should see that the agent runs until it reaches the interrupt() call, at which point it pauses and waits for human input. Resume the agent with a Command(resume=...) to continue based on human input. ```python from langgraph.types import Command for chunk in agent.stream( # highlight-next-line Command(resume={\"type\": \"accept\"}), # (1)! # Command(resume={\"type\": \"edit\", \"args\": {\"hotel_name\": \"McKittrick Hotel\"}}), config ): print(chunk) print(\"\\n\") ``` The [interrupt function][langgraph.types.interrupt] is used in conjunction with the [Command][langgraph.types.Command] object to resume the graph with a value provided by the human. "],"reference":"The InMemorySaver in LangGraph serves as a checkpointer that stores the agent's state in memory, enabling both short-term memory and human-in-the-loop capabilities. Short-term memory allows agents to track ongoing conversations by maintaining message history within a session, which is crucial for multi-turn interactions. When an agent is created, the InMemorySaver is passed as a checkpointer, allowing it to persist its state across invocations. This is particularly important when a unique thread_id is provided in the configuration, as it enables the agent to continue conversations seamlessly. In the context of human-in-the-loop features, the InMemorySaver also allows the agent to pause execution for human input, ensuring that sensitive tool calls can be reviewed and approved before proceeding. This combination of functionalities ensures that the agent can maintain context and facilitate interactive workflows effectively.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How can the langgraph-swarm library facilitate communication between agents in a multi-agent system, and how does this differ from the langgraph-supervisor library in terms of agent coordination?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags Multi-agent A single agent might struggle if it needs to specialize in multiple domains or manage many tools. To tackle this, you can break your agent into smaller, independent agents and composing them into a multi-agent system. In multi-agent systems, agents need to communicate between each other. They do so via handoffs \u2014 a primitive that describes which agent to hand control to and the payload to send to that agent. Two of the most popular multi-agent architectures are: supervisor \u2014 individual agents are coordinated by a central supervisor agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements. swarm \u2014 agents dynamically hand off control to one another based on their specializations. The system remembers which agent was last active, ensuring that on subsequent interactions, the conversation resumes with that agent. Supervisor Supervisor Use langgraph-supervisor library to create a supervisor multi-agent system: bash pip install langgraph-supervisor ```python from langchain_openai import ChatOpenAI from langgraph.prebuilt import create_react_agent highlight-next-line from langgraph_supervisor import create_supervisor def book_hotel(hotel_name: str): \"\"\"Book a hotel\"\"\" return f\"Successfully booked a stay at {hotel_name}.\" def book_flight(from_airport: str, to_airport: str): \"\"\"Book a flight\"\"\" return f\"Successfully booked a flight from {from_airport} to {to_airport}.\" flight_assistant = create_react_agent( model=\"openai:gpt-4o\", tools=[book_flight], prompt=\"You are a flight booking assistant\", # highlight-next-line name=\"flight_assistant\" ) hotel_assistant = create_react_agent( model=\"openai:gpt-4o\", tools=[book_hotel], prompt=\"You are a hotel booking assistant\", # highlight-next-line name=\"hotel_assistant\" ) highlight-next-line supervisor = create_supervisor( agents=[flight_assistant, hotel_assistant], model=ChatOpenAI(model=\"gpt-4o\"), prompt=( \"You manage a hotel booking assistant and a\" \"flight booking assistant. Assign work to them.\" ) ).compile() for chunk in supervisor.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\" } ] } ): print(chunk) print(\"\\n\") ``` Swarm Swarm Use langgraph-swarm library to create a swarm multi-agent system: bash pip install langgraph-swarm ```python from langgraph.prebuilt import create_react_agent highlight-next-line from langgraph_swarm import create_swarm, create_handoff_tool transfer_to_hotel_assistant = create_handoff_tool( agent_name=\"hotel_assistant\", description=\"Transfer user to the hotel-booking assistant.\", ) transfer_to_flight_assistant = create_handoff_tool( agent_name=\"flight_assistant\", description=\"Transfer user to the flight-booking assistant.\", ) flight_assistant = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", # highlight-next-line tools=[book_flight, transfer_to_hotel_assistant], prompt=\"You are a flight booking assistant\", # highlight-next-line name=\"flight_assistant\" ) hotel_assistant = create_react_agent( model=\"anthropic:claude-3-5-sonnet-latest\", # highlight-next-line tools=[book_hotel, transfer_to_flight_assistant], prompt=\"You are a hotel booking assistant\", # highlight-next-line name=\"hotel_assistant\" ) highlight-next-line swarm = create_swarm( agents=[flight_assistant, hotel_assistant], default_active_agent=\"flight_assistant\" ).compile() for chunk in swarm.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\" } ] } ): print(chunk) print(\"\\n\") ``` Handoffs A common pattern in multi-agent interactions is handoffs, where one agent hands off control to another. Handoffs allow you to specify: destination: target agent to navigate to payload: information to pass to that agent This is used both by langgraph-supervisor (supervisor hands off to individual agents) and langgraph-swarm (an individual agent can hand off to other agents). To implement handoffs with create_react_agent, you need to: Create a special tool that can transfer control to a different agent python def transfer_to_bob(): \"\"\"Transfer to bob.\"\"\" return Command( # name of the agent (node) to go to # highlight-next-line goto=\"bob\", # data to send to the agent # highlight-next-line update={\"messages\": [...]}, # indicate to LangGraph that we need to navigate to # agent node in a parent graph # highlight-next-line graph=Command.PARENT, ) Create individual agents that have access to handoff tools: python flight_assistant = create_react_agent( ..., tools=[book_flight, transfer_to_hotel_assistant] ) hotel_assistant = create_react_agent( ..., tools=[book_hotel, transfer_to_flight_assistant] ) Define a parent graph that contains individual agents as nodes: python from langgraph.graph import StateGraph, MessagesState multi_agent_graph = ( StateGraph(MessagesState) .add_node(flight_assistant) .add_node(hotel_assistant) ... ) Putting this together, here is how you can implement a simple multi-agent system with two agents \u2014 a flight booking assistant and a hotel booking assistant: ```python from typing import Annotated from langchain_core.tools import tool, InjectedToolCallId from langgraph.prebuilt import create_react_agent, InjectedState from langgraph.graph import StateGraph, START, MessagesState from langgraph.types import Command def create_handoff_tool(*, agent_name: str, description: str | None = None): name = f\"transfer_to_{agent_name}\" description = description or f\"Transfer to {agent_name}\" @tool(name, description=description) def handoff_tool( # highlight-next-line state: Annotated[MessagesState, InjectedState], # (1)! # highlight-next-line tool_call_id: Annotated[str, InjectedToolCallId], ) -> Command: tool_message = { \"role\": \"tool\", \"content\": f\"Successfully transferred to {agent_name}\", \"name\": name, \"tool_call_id\": tool_call_id, } return Command( # (2)! # highlight-next-line goto=agent_name, # (3)! # highlight-next-line update={\"messages\": state[\"messages\"] + [tool_message]}, # (4)! # highlight-next-line graph=Command.PARENT, # (5)! ) return handoff_tool Handoffs transfer_to_hotel_assistant = create_handoff_tool( agent_name=\"hotel_assistant\", description=\"Transfer user to the hotel-booking assistant.\", ) transfer_to_flight_assistant = create_handoff_tool( agent_name=\"flight_assistant\", description=\"Transfer user to the flight-booking assistant.\", )","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"The langgraph-swarm library facilitates communication between agents in a multi-agent system by allowing agents to dynamically hand off control to one another based on their specializations. This means that the system remembers which agent was last active, ensuring that subsequent interactions resume with that agent. In contrast, the langgraph-supervisor library coordinates individual agents through a central supervisor agent, which controls all communication flow and task delegation. The supervisor makes decisions about which agent to invoke based on the current context and task requirements, leading to a more structured approach to agent coordination.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How can I use LangChain to create tools that encapsulate functions and their input schemas, and what is the role of model fallbacks in this process?","reference_contexts":["<1-hop>\n\nAdding model fallbacks You can add a fallback to a different model or a different LLM provider using model.with_fallbacks([...]): === \"init_chat_model\" ```python from langchain.chat_models import init_chat_model model_with_fallbacks = ( init_chat_model(\"anthropic:claude-3-5-haiku-latest\") # highlight-next-line .with_fallbacks([ init_chat_model(\"openai:gpt-4.1-mini\"), ]) ) ``` === \"ChatModel\" ```python from langchain_anthropic import ChatAnthropic from langchain_openai import ChatOpenAI model_with_fallbacks = ( ChatAnthropic(model=\"claude-3-5-haiku-latest\") # highlight-next-line .with_fallbacks([ ChatOpenAI(model=\"gpt-4.1-mini\"), ]) ) ``` See this guide for more information on model fallbacks. Additional resources Model integration directory Universal initialization with init_chat_model","<2-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags Tools Tools are a way to encapsulate a function and its input schema in a way that can be passed to a chat model that supports tool calling. This allows the model to request the execution of this function with specific inputs. You can either define your own tools or use prebuilt integrations that LangChain provides. Define simple tools You can pass a vanilla function to create_react_agent to use as a tool: ```python from langgraph.prebuilt import create_react_agent def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * b create_react_agent( model=\"anthropic:claude-3-7-sonnet\", tools=[multiply] ) ``` create_react_agent automatically converts vanilla functions to LangChain tools. Customize tools For more control over tool behavior, use the @tool decorator: ```python highlight-next-line from langchain_core.tools import tool highlight-next-line @tool(\"multiply_tool\", parse_docstring=True) def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers. Args: a: First operand b: Second operand \"\"\" return a * b ``` You can also define a custom input schema using Pydantic: ```python from pydantic import BaseModel, Field class MultiplyInputSchema(BaseModel): \"\"\"Multiply two numbers\"\"\" a: int = Field(description=\"First operand\") b: int = Field(description=\"Second operand\") highlight-next-line @tool(\"multiply_tool\", args_schema=MultiplyInputSchema) def multiply(a: int, b: int) -> int: return a * b ``` For additional customization, refer to the custom tools guide. Hide arguments from the model Some tools require runtime-only arguments (e.g., user ID or session context) that should not be controllable by the model. You can put these arguments in the state or config of the agent, and access this information inside the tool: ```python from langgraph.prebuilt import InjectedState from langgraph.prebuilt.chat_agent_executor import AgentState from langchain_core.runnables import RunnableConfig def my_tool( # This will be populated by an LLM tool_arg: str, # access information that's dynamically updated inside the agent # highlight-next-line state: Annotated[AgentState, InjectedState], # access static data that is passed at agent invocation # highlight-next-line config: RunnableConfig, ) -> str: \"\"\"My tool.\"\"\" do_something_with_state(state[\"messages\"]) do_something_with_config(config) ... ``` Disable parallel tool calling Some model providers support executing multiple tools in parallel, but allow users to disable this feature. For supported providers, you can disable parallel tool calling by setting parallel_tool_calls=False via the model.bind_tools() method: ```python from langchain.chat_models import init_chat_model def add(a: int, b: int) -> int: \"\"\"Add two numbers\"\"\" return a + b def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * b model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0) tools = [add, multiply] agent = create_react_agent( # disable parallel tool calls # highlight-next-line model=model.bind_tools(tools, parallel_tool_calls=False), tools=tools ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 3 + 5 and 4 * 7?\"}]} ) ``` Return tool results directly Use return_direct=True to return tool results immediately and stop the agent loop: ```python from langchain_core.tools import tool highlight-next-line @tool(return_direct=True) def add(a: int, b: int) -> int: \"\"\"Add two numbers\"\"\" return a + b agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[add] ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 3 + 5?\"}]} ) ``` Force tool use To force the agent to use specific tools, you can set the tool_choice option in model.bind_tools(): ```python from langchain_core.tools import tool highlight-next-line @tool(return_direct=True) def greet(user_name: str) -> int: \"\"\"Greet user.\"\"\" return f\"Hello {user_name}!\" tools = [greet] agent = create_react_agent( # highlight-next-line model=model.bind_tools(tools, tool_choice={\"type\": \"tool\", \"name\": \"greet\"}), tools=tools ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Hi, I am Bob\"}]} ) ``` !!! Warning \"Avoid infinite loops\" Forcing tool usage without stopping conditions can create infinite loops. Use one of the following safeguards: - Mark the tool with [`return_direct=True`](#return-tool-results-directly) to end the loop after execution. - Set [`recursion_limit`](..\/concepts\/low_level.md#recursion-limit) to restrict the number of execution steps."],"reference":"You can use LangChain to create tools by encapsulating a function and its input schema, allowing the model to request execution with specific inputs. For example, you can define a simple tool by passing a vanilla function to create_react_agent. Additionally, you can customize tools using the @tool decorator and define a custom input schema with Pydantic. Model fallbacks can be added to ensure that if one model fails, another can be used, which is done using model.with_fallbacks([...]). This allows for greater flexibility and reliability in function execution.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How can LangGraph's error handling capabilities be integrated with model fallbacks, specifically using the models 'anthropic:claude-3-5-haiku-latest' and 'anthropic:claude-3-7-sonnet-latest'?","reference_contexts":["<1-hop>\n\nHandle tool errors By default, the agent will catch all exceptions raised during tool calls and will pass those as tool messages to the LLM. To control how the errors are handled, you can use the prebuilt [ToolNode][langgraph.prebuilt.tool_node.ToolNode] \u2014 the node that executes tools inside create_react_agent \u2014 via its handle_tool_errors parameter: === \"Enable error handling (default)\" ```python from langgraph.prebuilt import create_react_agent def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" if a == 42: raise ValueError(\"The ultimate error\") return a * b # Run with error handling (default) agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[multiply] ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]} ) ``` === \"Disable error handling\" ```python from langgraph.prebuilt import create_react_agent, ToolNode def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" if a == 42: raise ValueError(\"The ultimate error\") return a * b # highlight-next-line tool_node = ToolNode( [multiply], # highlight-next-line handle_tool_errors=False # (1)! ) agent_no_error_handling = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=tool_node ) agent_no_error_handling.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]} ) ``` 1. This disables error handling (enabled by default). See all available strategies in the [API reference][langgraph.prebuilt.tool_node.ToolNode]. === \"Custom error handling\" ```python from langgraph.prebuilt import create_react_agent, ToolNode def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" if a == 42: raise ValueError(\"The ultimate error\") return a * b # highlight-next-line tool_node = ToolNode( [multiply], # highlight-next-line handle_tool_errors=( \"Can't use 42 as a first operand, you must switch operands!\" # (1)! ) ) agent_custom_error_handling = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=tool_node ) agent_custom_error_handling.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]} ) ``` 1. This provides a custom message to send to the LLM in case of an exception. See all available strategies in the [API reference][langgraph.prebuilt.tool_node.ToolNode]. See [API reference][langgraph.prebuilt.tool_node.ToolNode] for more information on different tool error handling options. Working with memory LangGraph allows access to short-term and long-term memory from tools. See Memory guide for more information on: how to read from and write to short-term memory how to read from and write to long-term memory Prebuilt tools You can use prebuilt tools from model providers by passing a dictionary with tool specs to the tools parameter of create_react_agent. For example, to use the web_search_preview tool from OpenAI: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"openai:gpt-4o-mini\", tools=[{\"type\": \"web_search_preview\"}] ) response = agent.invoke( {\"messages\": [\"What was a positive news story from today?\"]} ) ``` Additionally, LangChain supports a wide range of prebuilt tool integrations for interacting with APIs, databases, file systems, web data, and more. These tools extend the functionality of agents and enable rapid development. You can browse the full list of available integrations in the LangChain integrations directory. Some commonly used tool categories include: Search: Bing, SerpAPI, Tavily Code interpreters: Python REPL, Node.js REPL Databases: SQL, MongoDB, Redis Web data: Web scraping and browsing APIs: OpenWeatherMap, NewsAPI, and others These integrations can be configured and added to your agents using the same tools parameter shown in the examples above.","<2-hop>\n\nAdding model fallbacks You can add a fallback to a different model or a different LLM provider using model.with_fallbacks([...]): === \"init_chat_model\" ```python from langchain.chat_models import init_chat_model model_with_fallbacks = ( init_chat_model(\"anthropic:claude-3-5-haiku-latest\") # highlight-next-line .with_fallbacks([ init_chat_model(\"openai:gpt-4.1-mini\"), ]) ) ``` === \"ChatModel\" ```python from langchain_anthropic import ChatAnthropic from langchain_openai import ChatOpenAI model_with_fallbacks = ( ChatAnthropic(model=\"claude-3-5-haiku-latest\") # highlight-next-line .with_fallbacks([ ChatOpenAI(model=\"gpt-4.1-mini\"), ]) ) ``` See this guide for more information on model fallbacks. Additional resources Model integration directory Universal initialization with init_chat_model"],"reference":"LangGraph allows for effective error handling by enabling the agent to catch exceptions raised during tool calls and pass them as messages to the LLM. For instance, when using the model 'anthropic:claude-3-7-sonnet-latest', you can implement error handling by utilizing the prebuilt ToolNode with the handle_tool_errors parameter set to true. Additionally, you can enhance the model's capabilities by adding fallbacks to different models. For example, you can initialize a chat model with 'anthropic:claude-3-5-haiku-latest' and set up fallbacks to 'openai:gpt-4.1-mini'. This integration allows for a robust system that not only manages errors effectively but also ensures continuity in responses by switching to alternative models when necessary.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How can LangGraph and LangChain be utilized together to create an agent that performs mathematical operations like addition and multiplication?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags Tools Tools are a way to encapsulate a function and its input schema in a way that can be passed to a chat model that supports tool calling. This allows the model to request the execution of this function with specific inputs. You can either define your own tools or use prebuilt integrations that LangChain provides. Define simple tools You can pass a vanilla function to create_react_agent to use as a tool: ```python from langgraph.prebuilt import create_react_agent def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * b create_react_agent( model=\"anthropic:claude-3-7-sonnet\", tools=[multiply] ) ``` create_react_agent automatically converts vanilla functions to LangChain tools. Customize tools For more control over tool behavior, use the @tool decorator: ```python highlight-next-line from langchain_core.tools import tool highlight-next-line @tool(\"multiply_tool\", parse_docstring=True) def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers. Args: a: First operand b: Second operand \"\"\" return a * b ``` You can also define a custom input schema using Pydantic: ```python from pydantic import BaseModel, Field class MultiplyInputSchema(BaseModel): \"\"\"Multiply two numbers\"\"\" a: int = Field(description=\"First operand\") b: int = Field(description=\"Second operand\") highlight-next-line @tool(\"multiply_tool\", args_schema=MultiplyInputSchema) def multiply(a: int, b: int) -> int: return a * b ``` For additional customization, refer to the custom tools guide. Hide arguments from the model Some tools require runtime-only arguments (e.g., user ID or session context) that should not be controllable by the model. You can put these arguments in the state or config of the agent, and access this information inside the tool: ```python from langgraph.prebuilt import InjectedState from langgraph.prebuilt.chat_agent_executor import AgentState from langchain_core.runnables import RunnableConfig def my_tool( # This will be populated by an LLM tool_arg: str, # access information that's dynamically updated inside the agent # highlight-next-line state: Annotated[AgentState, InjectedState], # access static data that is passed at agent invocation # highlight-next-line config: RunnableConfig, ) -> str: \"\"\"My tool.\"\"\" do_something_with_state(state[\"messages\"]) do_something_with_config(config) ... ``` Disable parallel tool calling Some model providers support executing multiple tools in parallel, but allow users to disable this feature. For supported providers, you can disable parallel tool calling by setting parallel_tool_calls=False via the model.bind_tools() method: ```python from langchain.chat_models import init_chat_model def add(a: int, b: int) -> int: \"\"\"Add two numbers\"\"\" return a + b def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers.\"\"\" return a * b model = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0) tools = [add, multiply] agent = create_react_agent( # disable parallel tool calls # highlight-next-line model=model.bind_tools(tools, parallel_tool_calls=False), tools=tools ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 3 + 5 and 4 * 7?\"}]} ) ``` Return tool results directly Use return_direct=True to return tool results immediately and stop the agent loop: ```python from langchain_core.tools import tool highlight-next-line @tool(return_direct=True) def add(a: int, b: int) -> int: \"\"\"Add two numbers\"\"\" return a + b agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[add] ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's 3 + 5?\"}]} ) ``` Force tool use To force the agent to use specific tools, you can set the tool_choice option in model.bind_tools(): ```python from langchain_core.tools import tool highlight-next-line @tool(return_direct=True) def greet(user_name: str) -> int: \"\"\"Greet user.\"\"\" return f\"Hello {user_name}!\" tools = [greet] agent = create_react_agent( # highlight-next-line model=model.bind_tools(tools, tool_choice={\"type\": \"tool\", \"name\": \"greet\"}), tools=tools ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Hi, I am Bob\"}]} ) ``` !!! Warning \"Avoid infinite loops\" Forcing tool usage without stopping conditions can create infinite loops. Use one of the following safeguards: - Mark the tool with [`return_direct=True`](#return-tool-results-directly) to end the loop after execution. - Set [`recursion_limit`](..\/concepts\/low_level.md#recursion-limit) to restrict the number of execution steps."],"reference":"LangGraph can be used in conjunction with LangChain to create an agent that performs mathematical operations by defining tools as functions. For instance, you can create a tool for addition and another for multiplication using vanilla Python functions. The agent can be set up using the `create_react_agent` function from LangGraph, where you pass the defined tools to the agent. For example, you can define a function `add` to add two numbers and a function `multiply` to multiply two numbers. These functions can then be passed to the `create_react_agent` method, allowing the agent to invoke these tools based on user input.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"How does the configuration of an LLM with specific parameters relate to the user information management involving John Smith in LangGraph?","reference_contexts":["<1-hop>\n\nsubsequent tools or prompts. ```python from typing import Annotated from langchain_core.tools import InjectedToolCallId from langchain_core.runnables import RunnableConfig from langchain_core.messages import ToolMessage from langgraph.prebuilt import InjectedState, create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.types import Command class CustomState(AgentState): # highlight-next-line user_name: str def update_user_info( tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig ) -> Command: \"\"\"Look up and update user info.\"\"\" user_id = config[\"configurable\"].get(\"user_id\") name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\" # highlight-next-line return Command(update={ # highlight-next-line \"user_name\": name, # update the message history \"messages\": [ ToolMessage( \"Successfully looked up user information\", tool_call_id=tool_call_id ) ] }) def greet( # highlight-next-line state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Use this to greet the user once you found their info.\"\"\" user_name = state[\"user_name\"] return f\"Hello {user_name}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[update_user_info, greet], # highlight-next-line state_schema=CustomState ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} ) ``` For more details, see how to update state from tools. Long-term memory Use long-term memory to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information. To use long-term memory, you need to: Configure a store to persist data across invocations. Use the [get_store][langgraph.config.get_store] function to access the store from within tools or prompts. Read { #read-long-term } ```python title=\"A tool the agent can use to look up user information\" from langchain_core.runnables import RunnableConfig from langgraph.config import get_store from langgraph.prebuilt import create_react_agent from langgraph.store.memory import InMemoryStore highlight-next-line store = InMemoryStore() # (1)! highlight-next-line store.put( # (2)! (\"users\",), # (3)! \"user_123\", # (4)! { \"name\": \"John Smith\", \"language\": \"English\", } # (5)! ) def get_user_info(config: RunnableConfig) -> str: \"\"\"Look up user info.\"\"\" # Same as that provided to create_react_agent # highlight-next-line store = get_store() # (6)! user_id = config[\"configurable\"].get(\"user_id\") # highlight-next-line user_info = store.get((\"users\",), user_id) # (7)! return str(user_info.value) if user_info else \"Unknown user\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_user_info], # highlight-next-line store=store # (8)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} ) ``` The InMemoryStore is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you. For this example, we write some sample data to the store using the put method. Please see the [BaseStore.put][langgraph.store.base.BaseStore.put] API reference for more details. The first argument is the namespace. This is used to group related data together. In this case, we are using the users namespace to group user data. A key within the namespace. This example uses a user ID for the key. The data that we want to store for the given user. The get_store function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created. The get method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a StoreValue object, which contains the value and metadata about the value. The store is passed to the agent. This enables the agent to access the store when running tools. You can also use the get_store function to access the store from anywhere in your code. Write { #write-long-term } ```python title=\"Example of a tool that updates user information\" from typing_extensions import TypedDict from langgraph.config import get_store from langgraph.prebuilt import create_react_agent from langgraph.store.memory import InMemoryStore store = InMemoryStore() # (1)! class UserInfo(TypedDict): # (2)! name: str def save_user_info(user_info: UserInfo, config: RunnableConfig) -> str: # (3)! \"\"\"Save user info.\"\"\" # Same as that provided to create_react_agent # highlight-next-line store = get_store() # (4)! user_id = config[\"configurable\"].get(\"user_id\") # highlight-next-line store.put((\"users\",), user_id, user_info) # (5)! return \"Successfully saved user info.\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[save_user_info], # highlight-next-line store=store ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]}, # highlight-next-line config={\"configurable\": {\"user_id\": \"user_123\"}} # (6)! ) You can access the store directly to get the value store.get((\"users\",), \"user_123\").value ``` The InMemoryStore is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you. The UserInfo class is a TypedDict that defines the structure of the user information. The LLM will use this to format the response according to the schema. The save_user_info function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information. The get_store function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created. The put method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store. The user_id is passed in the config. This is used to identify the user whose information is being updated.","<2-hop>\n\n3. Configure an LLM To configure an LLM with specific parameters, such as temperature, use init_chat_model: ```python from langchain.chat_models import init_chat_model from langgraph.prebuilt import create_react_agent highlight-next-line model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line temperature=0 ) agent = create_react_agent( # highlight-next-line model=model, tools=[get_weather], ) ``` For more information on how to configure LLMs, see Models. 4. Add a custom prompt Prompts instruct the LLM how to behave. Add one of the following types of prompts: Static: A string is interpreted as a system message. Dynamic: A list of messages generated at runtime, based on input or configuration. === \"Static prompt\" Define a fixed prompt string or list of messages: ```python from langgraph.prebuilt import create_react_agent agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # A static prompt that never changes # highlight-next-line prompt=\"Never answer questions about the weather.\" ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` === \"Dynamic prompt\" Define a function that returns a message list based on the agent's state and configuration: ```python from langchain_core.messages import AnyMessage from langchain_core.runnables import RunnableConfig from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.prebuilt import create_react_agent # highlight-next-line def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]: # (1)! user_name = config[\"configurable\"].get(\"user_name\") system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\" return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"] agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line prompt=prompt ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config={\"configurable\": {\"user_name\": \"John Smith\"}} ) ``` 1. Dynamic prompts allow including non-message [context](.\/context.md) when constructing an input to the LLM, such as: - Information passed at runtime, like a `user_id` or API credentials (using `config`). - Internal agent state updated during a multi-step reasoning process (using `state`). Dynamic prompts can be defined as functions that take `state` and `config` and return a list of messages to send to the LLM. For more information, see Context. 5. Add memory To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing thread_id \u2014 a unique identifier for the conversation (session): ```python from langgraph.prebuilt import create_react_agent from langgraph.checkpoint.memory import InMemorySaver highlight-next-line checkpointer = InMemorySaver() agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line checkpointer=checkpointer # (1)! ) Run the agent highlight-next-line config = {\"configurable\": {\"thread_id\": \"1\"}} sf_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, # highlight-next-line config # (2)! ) ny_response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]}, # highlight-next-line config ) ``` checkpointer allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. Pass configuration with thread_id to be able to resume the same conversation on future agent invocations. When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver). Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input. For more information, see Memory. 6. Configure structured output To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field. ```python from pydantic import BaseModel from langgraph.prebuilt import create_react_agent class WeatherResponse(BaseModel): conditions: str agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", tools=[get_weather], # highlight-next-line response_format=WeatherResponse # (1)! ) response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) highlight-next-line response[\"structured_response\"] ``` When response_format is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response. To provide a system prompt to this LLM, use a tuple (prompt, schema), e.g., response_format=(prompt, WeatherResponse). !!! Note \"LLM post-processing\" Structured output requires an additional call to the LLM to format the response according to the schema. Next steps Deploy your agent locally Learn more about prebuilt agents LangGraph Platform quickstart"],"reference":"The configuration of an LLM with specific parameters, such as temperature, is essential for tailoring the model's responses in LangGraph. For instance, when managing user information, the agent can utilize the configured LLM to greet users like John Smith effectively. The agent can invoke functions to look up and update user information, ensuring that the correct name is used in interactions. This is achieved by integrating user-specific data, such as the name 'John Smith', into the agent's dynamic prompts, which instruct the LLM on how to address the user based on their stored information.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
{"user_input":"What are the steps to create an agent using the Anthropic model in LangGraph, and how does it differ from using the OpenAI model?","reference_contexts":["<1-hop>\n\nsearch: boost: 2 tags: - agent hide: - tags LangGraph quickstart This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably. Prerequisites Before you start this tutorial, ensure you have the following: An Anthropic API key 1. Install dependencies If you haven't already, install LangGraph and LangChain: pip install -U langgraph \"langchain[anthropic]\" !!! info LangChain is installed so the agent can call the [model](https:\/\/python.langchain.com\/docs\/integrations\/chat\/). 2. Create an agent To create an agent, use [create_react_agent][langgraph.prebuilt.chat_agent_executor.create_react_agent]: ```python from langgraph.prebuilt import create_react_agent def get_weather(city: str) -> str: # (1)! \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create_react_agent( model=\"anthropic:claude-3-7-sonnet-latest\", # (2)! tools=[get_weather], # (3)! prompt=\"You are a helpful assistant\" # (4)! ) Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page. Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page. Provide a list of tools for the model to use. Provide a system prompt (instructions) to the language model used by the agent.","<2-hop>\n\nsearch: boost: 2 tags: - anthropic - openai - agent hide: - tags Models This page describes how to configure the chat model used by an agent. Tool calling support To enable tool-calling agents, the underlying LLM must support tool calling. Compatible models can be found in the LangChain integrations directory. Specifying a model by name You can configure an agent with a model name string: === \"OpenAI\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" agent = create_react_agent( # highlight-next-line model=\"openai:gpt-4.1\", # other parameters ) ``` === \"Anthropic\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" agent = create_react_agent( # highlight-next-line model=\"anthropic:claude-3-7-sonnet-latest\", # other parameters ) ``` === \"Azure\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\" os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\" os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\" agent = create_react_agent( # highlight-next-line model=\"azure_openai:gpt-4.1\", # other parameters ) ``` === \"Google Gemini\" ```python import os from langgraph.prebuilt import create_react_agent os.environ[\"GOOGLE_API_KEY\"] = \"...\" agent = create_react_agent( # highlight-next-line model=\"google_genai:gemini-2.0-flash\", # other parameters ) ``` === \"AWS Bedrock\" ```python from langgraph.prebuilt import create_react_agent # Follow the steps here to configure your credentials: # https:\/\/docs.aws.amazon.com\/bedrock\/latest\/userguide\/getting-started.html agent = create_react_agent( # highlight-next-line model=\"bedrock_converse:anthropic.claude-3-5-sonnet-20240620-v1:0\", # other parameters ) ``` Using init_chat_model The init_chat_model utility simplifies model initialization with configurable parameters: === \"OpenAI\" ``` pip install -U \"langchain[openai]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" model = init_chat_model( \"openai:gpt-4.1\", temperature=0, # other parameters ) ``` === \"Anthropic\" ``` pip install -U \"langchain[anthropic]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" model = init_chat_model( \"anthropic:claude-3-5-sonnet-latest\", temperature=0, # other parameters ) ``` === \"Azure\" ``` pip install -U \"langchain[openai]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\" os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\" os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\" model = init_chat_model( \"azure_openai:gpt-4.1\", azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"], temperature=0, # other parameters ) ``` === \"Google Gemini\" ``` pip install -U \"langchain[google-genai]\" ``` ```python import os from langchain.chat_models import init_chat_model os.environ[\"GOOGLE_API_KEY\"] = \"...\" model = init_chat_model( \"google_genai:gemini-2.0-flash\", temperature=0, # other parameters ) ``` === \"AWS Bedrock\" ``` pip install -U \"langchain[aws]\" ``` ```python from langchain.chat_models import init_chat_model # Follow the steps here to configure your credentials: # https:\/\/docs.aws.amazon.com\/bedrock\/latest\/userguide\/getting-started.html model = init_chat_model( \"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_provider=\"bedrock_converse\", temperature=0, # other parameters ) ``` Refer to the API reference for advanced options. Using provider-specific LLMs If a model provider is not available via init_chat_model, you can instantiate the provider's model class directly. The model must implement the BaseChatModel interface and support tool calling: ```python from langchain_anthropic import ChatAnthropic from langgraph.prebuilt import create_react_agent model = ChatAnthropic( model=\"claude-3-7-sonnet-latest\", temperature=0, max_tokens=2048 ) agent = create_react_agent( # highlight-next-line model=model, # other parameters ) ``` !!! note \"Illustrative example\" The example above uses `ChatAnthropic`, which is already supported by `init_chat_model`. This pattern is shown to illustrate how to manually instantiate a model not available through init_chat_model. Disable streaming To disable streaming of the individual LLM tokens, set disable_streaming=True when initializing the model: === \"init_chat_model\" ```python from langchain.chat_models import init_chat_model model = init_chat_model( \"anthropic:claude-3-7-sonnet-latest\", # highlight-next-line disable_streaming=True ) ``` === \"ChatModel\" ```python from langchain_anthropic import ChatAnthropic model = ChatAnthropic( model=\"claude-3-7-sonnet-latest\", # highlight-next-line disable_streaming=True ) ``` Refer to the API reference for more information on disable_streaming"],"reference":"To create an agent using the Anthropic model in LangGraph, follow these steps: First, ensure you have an Anthropic API key. Then, install the necessary dependencies by running 'pip install -U langgraph \"langchain[anthropic]\"'. Next, you can create an agent using the 'create_react_agent' function from the 'langgraph.prebuilt' module. For example, you would import the function and define a tool for the agent to use, such as a function to get weather information. You would specify the model as 'anthropic:claude-3-7-sonnet-latest' when creating the agent. In contrast, to create an agent using the OpenAI model, you would set the model parameter to 'openai:gpt-4.1' and ensure you have the OpenAI API key set in your environment. The main difference lies in the model name and the API key used for authentication, as well as the specific libraries installed for each model.","synthesizer_name":"multi_hop_specific_query_synthesizer"}
